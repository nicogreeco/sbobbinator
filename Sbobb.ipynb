{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yt_dlp\n",
    "import assemblyai as aai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "###############################################################################\n",
    "# 2. ASSEMBLYAI TRANSCRIPTION\n",
    "###############################################################################\n",
    "\n",
    "def extract_audio_from_youtube(youtube_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the best available audio URL (m4a format) from the given YouTube URL.\n",
    "    \"\"\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"cookiesfrombrowser\": ('firefox',)\n",
    "    }\n",
    "    \n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=False)\n",
    "\n",
    "    # Iterate over formats in reverse (best quality first) and pick one with audio only\n",
    "    for fmt in reversed(info.get(\"formats\", [])):\n",
    "        if fmt.get(\"acodec\") != \"none\" and fmt.get(\"ext\") == \"m4a\":\n",
    "            return fmt[\"url\"]\n",
    "\n",
    "def transcribe_audio_assemblyai(audio_url_or_path: str, language_code: str = \"en\") -> aai.Transcript:\n",
    "    \"\"\"\n",
    "    Transcribe the audio from the given URL or local file path using AssemblyAI.\n",
    "    If a YouTube URL is provided, it automatically extracts the audio URL.\n",
    "    Returns the transcript object.\n",
    "    \"\"\"\n",
    "    # If the input appears to be a YouTube URL, extract the audio URL.\n",
    "    if \"youtube.com\" in audio_url_or_path or \"youtu.be\" in audio_url_or_path:\n",
    "        print(\"YouTube URL detected. Extracting audio URL...\")\n",
    "        audio_url_or_path = extract_audio_from_youtube(audio_url_or_path)\n",
    "        print(f\"YouTube video audio URL: {audio_url_or_path}\")\n",
    "    \n",
    "    # Set up AssemblyAI\n",
    "    aai.settings.api_key = ASSEMBLYAI_API_KEY\n",
    "    config = aai.TranscriptionConfig(language_code=language_code)\n",
    "    transcriber = aai.Transcriber(config=config)\n",
    "\n",
    "    # Start transcription\n",
    "    transcript = transcriber.transcribe(audio_url_or_path)\n",
    "\n",
    "    # Poll for completion\n",
    "    while transcript.status not in ['completed', 'error']:\n",
    "        print(f\"Transcription status: {transcript.status}. Waiting...\")\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "        transcript = transcriber.get_transcription(transcript.id)\n",
    "\n",
    "    # Check for errors\n",
    "    if transcript.status == aai.TranscriptStatus.error:\n",
    "        raise RuntimeError(f\"Transcription failed: {transcript.error}\")\n",
    "\n",
    "    return transcript\n",
    "\n",
    "###############################################################################\n",
    "# 3. CHUNKING THE TRANSCRIPT\n",
    "###############################################################################\n",
    "\n",
    "def chunk_text_by_paragraphs(transcript: aai.Transcript, chunk_word_target: int = 600) -> list:\n",
    "    \"\"\"\n",
    "    Splits the transcript into chunks based on its paragraphs, aiming for about\n",
    "    `chunk_word_target` words each. Accumulates paragraphs until the target is reached.\n",
    "    \n",
    "    Returns a list of textual chunks (strings).\n",
    "    \"\"\"\n",
    "    paragraphs = transcript.get_paragraphs()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_text = paragraph.text.strip()\n",
    "        if not paragraph_text:\n",
    "            continue  # Skip empty paragraphs\n",
    "\n",
    "        paragraph_word_count = len(paragraph_text.split())\n",
    "\n",
    "        # If adding this paragraph exceeds the target and current chunk is not empty, create a new chunk\n",
    "        if (current_word_count + paragraph_word_count) > chunk_word_target and current_chunk:\n",
    "            chunk = \"\\n\".join(current_chunk)\n",
    "            chunks.append(chunk)\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "\n",
    "        # Add the paragraph to the current chunk\n",
    "        current_chunk.append(paragraph_text)\n",
    "        current_word_count += paragraph_word_count\n",
    "\n",
    "    # Add any remaining paragraphs as the last chunk\n",
    "    if current_chunk:\n",
    "        chunk = \"\\n\".join(current_chunk)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "###############################################################################\n",
    "# 4. OPENAI REWRITING (PAGE-BY-PAGE)\n",
    "###############################################################################\n",
    "def rewrite_chunk_with_openai(chunk_text: str,\n",
    "                              model: str = OPENAI_MODEL,\n",
    "                              prev_summary: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Sends a chunk of text to OpenAI for rewriting in a 'professorial' register.\n",
    "\n",
    "    Optionally includes `prev_summary` – a short summary of all previously\n",
    "    processed chunks – as context for better continuity across chunks.\n",
    "\n",
    "    Returns the revised chunk as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build system prompt with instructions\n",
    "    system_prompt = (\n",
    "        \"You are an expert in rewriting transcripts with a professorial register. \"\n",
    "        \"You will receive fragments of a university lesson transcript generated \"\n",
    "        \"from an audio recording. Your role is to correct grammar, punctuation, \"\n",
    "        \"and spelling, fix words that may be misrecognized, remove filler words, \"\n",
    "        \"and elevate the text to an academic standard. Output only the revised \"\n",
    "        \"transcript text in plain text, without titles, markdown, or other formatting. \"\n",
    "        \"Maintain context as if it were in medias res.\"\n",
    "    )\n",
    "\n",
    "    # Build user prompt with the chunk, plus the short summary of prior chunks\n",
    "    # The summary is for context only; it helps the model keep track of earlier topics.\n",
    "    if prev_summary:\n",
    "        user_prompt = (\n",
    "            f\"Here is a short summary of what has come before:\\n{prev_summary}\\n\\n\"\n",
    "            f\"Now, rewrite the following chunk:\\n\\n{chunk_text}\\n\\n\"\n",
    "            \"Output only the revised text. Do not add extra commentary or formatting.\"\n",
    "        )\n",
    "    else:\n",
    "        user_prompt = (\n",
    "            f\"Now, rewrite the following chunk:\\n\\n{chunk_text}\\n\\n\"\n",
    "            \"Output only the revised text. Do not add extra commentary or formatting.\"\n",
    "        )\n",
    "\n",
    "    # Call OpenAI ChatCompletion using the client\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,  # Keep temperature low for consistent rewriting\n",
    "        max_tokens=1500,   # Enough tokens to handle rewriting a ~600-word chunk\n",
    "    )\n",
    "\n",
    "    revised_text = response.choices[0].message.content\n",
    "    return revised_text.strip()\n",
    "\n",
    "def summarize_text_with_openai(text: str,\n",
    "                               model: str = \"chatgpt-4o-mini\") -> str:\n",
    "    \"\"\"\n",
    "    Summarizes the given text in a couple of sentences to maintain context\n",
    "    for future rewriting chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a concise and precise summarizer. Summarize the following text \"\n",
    "        \"in one sentence, focusing on the key ideas. Keep it short. Do not referes \"\n",
    "        \"to the text itself, just provide a single sentence that capture the kay ideas.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"Text to summarize:\\n{text}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=200,\n",
    "    )\n",
    "\n",
    "    summary = response.choices[0].message.content\n",
    "    return summary.strip()\n",
    "\n",
    "def get_word_count(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the word count of the given text.\n",
    "    \"\"\"\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1. CONFIGURATION\n",
    "###############################################################################\n",
    "\n",
    "# Define the job name (for output file)\n",
    "job_name = input(\"Job name: \")\n",
    "\n",
    "# Load environment variables from config.env\n",
    "load_dotenv('./config.env')\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "ASSEMBLYAI_API_KEY = os.getenv(\"ASSEMBLYAI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate API keys\n",
    "if not ASSEMBLYAI_API_KEY:\n",
    "    raise ValueError(\"ASSEMBLYAI_API_KEY not found in config.env\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in config.env\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "# client = OpenAI(api_key=\"<DeepSeek API Key>\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# Define OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"  # Ensure this model is accessible with your API key\n",
    "\n",
    "# Define chunking parameters\n",
    "CHUNK_WORD_TARGET = 500  # Target words per chunk\n",
    "MAX_SUMMARY_WORDS = 300  # Maximum words in running summary before summarization\n",
    "ENABLE_SUMMARY_SUMMARIZATION = True  # Toggle for summary summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Encodings: locale UTF-8, fs utf-8, pref UTF-8, out UTF-8 (No ANSI), error UTF-8 (No ANSI), screen UTF-8 (No ANSI)\n",
      "[debug] yt-dlp version stable@2025.02.19 from yt-dlp/yt-dlp [4985a4041] (pip) API\n",
      "[debug] params: {'cookiesfrombrowser': ('firefox',), 'verbose': 'True', 'compat_opts': set(), 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-us,en;q=0.5', 'Sec-Fetch-Mode': 'navigate'}}\n",
      "[debug] Python 3.13.0 (CPython x86_64 64bit) - Linux-6.11.0-19-generic-x86_64-with-glibc2.39 (OpenSSL 3.0.13 30 Jan 2024, glibc 2.39)\n",
      "[debug] exe versions: none\n",
      "[debug] Optional libraries: brotli-1.0.9, certifi-2024.08.30, requests-2.32.3, sqlite3-3.45.3, urllib3-2.2.3, websockets-14.1\n",
      "[debug] Proxy map: {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing audio... please wait.\n",
      "YouTube URL detected. Extracting audio URL...\n",
      "Extracting cookies from firefox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Extracting cookies from: \"/home/nicola/snap/firefox/common/.mozilla/firefox/jnaig3c6.default/cookies.sqlite\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1795 cookies from firefox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Request Handlers: urllib, requests, websockets\n",
      "[debug] Loaded 1841 extractors\n",
      "[debug] [youtube] Found YouTube account cookies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=28s&pp=ygUHTUlUIGxsbQ%3D%3D\n",
      "[youtube] 9vM4p9NN0Ts: Downloading webpage\n",
      "[youtube] 9vM4p9NN0Ts: Downloading tv client config\n",
      "[youtube] 9vM4p9NN0Ts: Downloading player 82345d49\n",
      "[youtube] 9vM4p9NN0Ts: Downloading tv player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Loading youtube-nsig.82345d49 from cache\n",
      "[debug] [youtube] Decrypted nsig mMRyRm7PI5TMRguQ => EVTbR_tjv5BhVw\n",
      "[debug] Loading youtube-nsig.82345d49 from cache\n",
      "[debug] [youtube] Decrypted nsig JJBMiYw7m5KKx6oj => GmEPKBrTsj2iig\n",
      "[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec, channels, acodec, lang, proto\n",
      "[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec, channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n",
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n",
      "[debug] Default format spec: best/bestvideo+bestaudio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube video audio URL: https://rr5---sn-5hne6nzd.googlevideo.com/videoplayback?expire=1742399055&ei=75HaZ8mAC9K-i9oPr7voqQ4&ip=213.39.100.109&id=o-AN_dgsnlT3k3QcyMDUvUAkdIedenddyiGxyOosREqalk&itag=140&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1742377455%2C&mh=_J&mm=31%2C29&mn=sn-5hne6nzd%2Csn-5hneknes&ms=au%2Crdu&mv=m&mvi=5&pl=24&rms=au%2Cau&initcwndbps=3118750&siu=1&bui=AccgBcNeEKG0eSnPxcp1QNy1F7Sth8wBfOrRa83Ra8REqFbXKP-UZPd4hE4gTMz3Qz7mpf1j&vprv=1&svpuc=1&mime=audio%2Fmp4&ns=DOkXPhUqNhxiUSyhLRvf6XUQ&rqh=1&gir=yes&clen=101484952&dur=6270.687&lmt=1724828401738727&mt=1742377253&fvip=1&keepalive=yes&lmw=1&c=TVHTML5&sefc=1&txp=5432434&n=GmEPKBrTsj2iig&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Csiu%2Cbui%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIgN3JiVgXsegfG8cvmKEphVgrAaF-Zm4m7REYBxo2Po0kCIQCVAvkFf_SEWj_jPlqmZ_U3kipvkuTRzvQoJHXCVo2YtQ%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AFVRHeAwRAIgU25u2cpN0j1qir9UiGSHEfTL3lPLoUmPQDz1Q4WsEnMCIDdF3E-NzSM4dfUSBK_hMGmM-aU7iOHanx3SlCwqHvDh\n"
     ]
    }
   ],
   "source": [
    "# 1) Transcribe audio\n",
    "# You can point to a local file, remote URL or a YouTube video. E.g.:\n",
    "# audio_source = \"https://assembly.ai/path_to_your_audio_file.mp3\"\n",
    "# or\n",
    "# audio_source = \"./local_file.mp3\"\n",
    "# or\n",
    "# audio_source = \"https://www.youtube.com/watch?v=YOUR_VIDEO\"\n",
    "audio_source = input('Path to audio file: ')  # Replace with your audio file path or URL\n",
    "\n",
    "print(\"Transcribing audio... please wait.\")\n",
    "try:\n",
    "    full_transcript_text = transcribe_audio_assemblyai(audio_source, language_code='en')\n",
    "except RuntimeError as e:\n",
    "    print(str(e))\n",
    "\n",
    "print(\"Transcription complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting transcript into chunks based on paragraphs...\n",
      "Created 9 chunk(s) of ~500 words each.\n"
     ]
    }
   ],
   "source": [
    "# 2) Chunk the transcript using paragraphs\n",
    "print(\"Splitting transcript into chunks based on paragraphs...\")\n",
    "chunks = chunk_text_by_paragraphs(full_transcript_text, chunk_word_target=CHUNK_WORD_TARGET)\n",
    "print(f\"Created {len(chunks)} chunk(s) of ~{CHUNK_WORD_TARGET} words each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewriting chunk 1/9...\n",
      "Rewriting chunk 2/9...\n",
      "Rewriting chunk 3/9...\n",
      "Rewriting chunk 4/9...\n",
      "Rewriting chunk 5/9...\n",
      "Rewriting chunk 6/9...\n",
      "Rewriting chunk 7/9...\n",
      "Rewriting chunk 8/9...\n",
      "Rewriting chunk 9/9...\n"
     ]
    }
   ],
   "source": [
    "# 3) For each chunk, rewrite with OpenAI\n",
    "final_rewritten_text = []\n",
    "running_summary = \"\"  # Will accumulate short summaries of prior chunks\n",
    "\n",
    "for i, chunk_text in enumerate(chunks, start=1):\n",
    "    print(\"Revriting the transcript ...\")\n",
    "    # print(f\"Rewriting chunk {i}/{len(chunks)}...\")\n",
    "\n",
    "    # Rewrite the chunk\n",
    "    try:\n",
    "        revised_text = rewrite_chunk_with_openai(\n",
    "            chunk_text=chunk_text,\n",
    "            model=OPENAI_MODEL,\n",
    "            prev_summary=running_summary\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error rewriting chunk {i}: {str(e)}\")\n",
    "        continue  # Skip to the next chunk\n",
    "\n",
    "    # Append the revised text to our final output\n",
    "    final_rewritten_text.append(revised_text)\n",
    "\n",
    "    # Summarize this revised chunk to update context\n",
    "    try:\n",
    "        chunk_summary = summarize_text_with_openai(revised_text, model=OPENAI_MODEL)\n",
    "        # print(f\"Summary for chunk {i}: {chunk_summary}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error summarizing chunk {i}: {str(e)}\")\n",
    "        chunk_summary = \"\"\n",
    "\n",
    "    # Append new summary to the running summary\n",
    "    \n",
    "    if ENABLE_SUMMARY_SUMMARIZATION:\n",
    "        running_summary += f\" {chunk_summary}\"\n",
    "        # Check if running_summary exceeds MAX_SUMMARY_WORDS\n",
    "        if get_word_count(running_summary) > MAX_SUMMARY_WORDS:\n",
    "            # print(\"Running summary exceeds maximum word limit. Summarizing the running summary...\")\n",
    "            try:\n",
    "                summarized_running_summary = summarize_text_with_openai(running_summary, model=OPENAI_MODEL)\n",
    "                running_summary = summarized_running_summary\n",
    "                # print(f\"Summarized running summary: {running_summary}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error summarizing running summary: {str(e)}\")\n",
    "                # Optionally, you can reset the running_summary or keep it as is\n",
    "    else:\n",
    "        running_summary += f\" {chunk_summary}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If one provides a large language model with the phrase \"Michael Jordan plays the sport of\" and prompts it to predict the subsequent words, a correct prediction of \"basketball\" would imply that within its hundreds of billions of parameters, the model possesses ingrained knowledge about a specific individual and his associated sport. Generally, anyone who has experimented with these models recognizes that they have memorized an extensive array of facts. Consequently, a pertinent question arises: how does this process function, and where are these facts stored?\\n\\nIn December of last year, several researchers from Google DeepMind published findings addressing this inquiry, utilizing the specific example of matching athletes to their respective sports. While a comprehensive mechanistic understanding of how facts are stored remains elusive, they presented intriguing partial results, including the overarching conclusion that these facts appear to reside within a particular segment of the networks, whimsically referred to as multilayer perceptrons, or MLPs for short. In the preceding chapters, we have delved into the intricacies of Transformers, the architecture that underpins large language models and many other contemporary artificial intelligence systems. In the most recent chapter, our focus was on a component known as Attention. The next step for us is to explore the details of the computations that occur within these multilayer perceptrons, which constitute another significant portion of the network.\\n\\nThe computations involved are relatively straightforward, particularly when contrasted with Attention. Essentially, they reduce to a pair of matrix multiplications with a simple operation in between. However, interpreting the implications of these computations is exceedingly complex. Our primary objective is to navigate through these computations and render them memorable. I would like to illustrate this process through a specific example of how one of these blocks could, at least in principle, store a concrete fact.\\n\\nSpecifically, it will be storing the fact that Michael Jordan plays basketball. I should note that the layout of this discussion is inspired by a conversation I had with one of the DeepMind researchers, Neil Nanda. For the most part, I will assume that you have either watched the last two chapters or possess a basic understanding of what a transformer is. However, refreshers are always beneficial. Thus, here is a brief reminder of the overall flow we have been studying: a model that is trained to receive a piece of text and predict what follows.',\n",
       " 'The input text is first divided into a series of tokens, which are small units that typically correspond to words or fragments of words. Each token is associated with a high-dimensional vector, which is essentially a lengthy list of numbers. This sequence of vectors then undergoes two types of operations: attention, which facilitates the exchange of information among the vectors, and multilayer perceptrons, the focus of today\\'s discussion. Additionally, there is a normalization step interspersed after this sequence of vectors has traversed numerous iterations of both operations.\\n\\nBy the conclusion of this process, the objective is for each vector to have absorbed sufficient information, both from the context provided by the surrounding words in the input and from the general knowledge embedded in the model weights during training, enabling it to predict the subsequent token. A key concept to keep in mind is that all of these vectors exist within an extremely high-dimensional space. In this context, different directions can encode various types of meaning. A classic example I often reference is the relationship between the embeddings of \"woman\" and \"man.\" If one subtracts the embedding of \"man\" from that of \"woman\" and adds this vector to another masculine noun, such as \"uncle,\" one arrives at a point very close to the corresponding feminine noun. This illustrates how this particular direction encodes gender information.\\n\\nThe premise is that numerous other distinct directions within this high-dimensional space may correspond to additional features that the model seeks to represent in a transformer. These vectors do not merely encapsulate the meaning of a single word; rather, as they progress through the network, they acquire a more nuanced meaning based on the surrounding context and the model\\'s knowledge. Ultimately, each vector must convey information that extends well beyond the meaning of an individual word, as it must be adequate to predict what follows next. We have already observed how attention blocks enable the incorporation of context.\\n\\nHowever, the majority of the model parameters reside within the multilayer perceptron blocks. One hypothesis regarding their function is that they provide additional capacity for storing factual information. As previously mentioned, today\\'s lesson will focus on a concrete toy example illustrating how the model could store the fact that Michael Jordan plays basketball. This example necessitates that we make a few assumptions about the high-dimensional space. First, we will assume that one direction represents the concept of the first name \"Michael,\" while another nearly perpendicular direction represents the concept of the last name \"Jordan.\"',\n",
       " 'A third direction will represent the concept of basketball. Specifically, if one examines the network and extracts one of the vectors being processed, a dot product of one with the direction corresponding to the first name \"Michael\" indicates that the vector encodes the idea of a person with that first name. Conversely, a dot product of zero or negative would suggest that the vector does not align with that direction. For simplicity, we will set aside the pertinent question of what it might signify if that dot product were greater than one.\\n\\nSimilarly, the dot product with the other directions would indicate whether the vector represents the last name \"Jordan\" or the concept of basketball. If a vector is intended to represent the full name \"Michael Jordan,\" then its dot product with both of these directions must equal one, given that the text \"Michael Jordan\" comprises two distinct tokens. This necessitates the assumption that an earlier attention block has successfully transmitted information to the second of these two vectors, ensuring that it can encode both names under these assumptions.\\n\\nNow, let us delve into the core of the lesson: what occurs within a multilayer perceptron? One might envision a sequence of vectors flowing into the block, each originally associated with one of the tokens from the input text. Each individual vector from that sequence undergoes a brief series of operations.\\n\\nWe will unpack these operations shortly, and ultimately, we will obtain another vector of the same dimension. This resultant vector will be added to the original vector that flowed in, and this sum constitutes the output. This sequence of operations is applied to every vector in the sequence associated with each token in the input, and it occurs in parallel. Notably, the vectors do not interact with one another during this step; each operates independently.\\n\\nThis independence simplifies the process for us, as understanding what happens to just one of the vectors through this block effectively allows us to comprehend the transformations applied to all of them. When I state that this block encodes the fact that Michael Jordan plays basketball, I mean that if a vector representing the first name \"Michael\" and the last name \"Jordan\" flows in, the sequence of computations will yield a result that incorporates the direction of \"basketball,\" which will then be added to the vector at that position. The initial step of this process involves multiplying that vector by a substantial matrix. This is a fundamental aspect of deep learning.',\n",
       " 'This matrix, like all the others we have examined, is populated with model parameters that are learned from data. One might conceptualize these parameters as a series of knobs and dials that are adjusted to determine the model\\'s behavior. A useful way to understand matrix multiplication is to envision each row of the matrix as an individual vector, with dot products being computed between these rows and the vector being processed, which I will denote as E for embedding. For instance, if the first row corresponds to the vector representing the first name \"Michael,\" then the first component of the output, derived from this dot product, would equal 1 if that vector encodes the name \"Michael,\" and 0 or a negative value otherwise. \\n\\nMoreover, consider the scenario in which the first row represents the combination of the first name \"Michael\" and the last name \"Jordan,\" which I will denote as Mj. When taking the dot product with the embedding E, the expression distributes nicely, yielding Me + Je. This indicates that the ultimate value would be 2 if the vector encodes the full name \"Michael Jordan,\" and otherwise it would be 1 or a value less than 1. This represents just one row in the matrix; one may consider all the other rows as simultaneously probing different aspects or features of the vector being processed.\\n\\nFrequently, this step also involves adding another vector to the output, which consists of model parameters learned from data. This additional vector is referred to as the bias. For our example, let us assume that the value of this bias in the first component is negative one, meaning our final output resembles the relevant dot product but is adjusted by subtracting one. One might reasonably inquire why this is the case. I would like you to assume that the model has learned this, and shortly, you will understand why this is a clean and effective approach.\\n\\nIf we have a value here that is positive if and only if our vector encodes the full name \"Michael Jordan,\" and is otherwise zero or negative, we can observe that the total number of rows in this matrix—akin to the number of questions being posed in the case of GPT-3, whose specifications we have been discussing—is just under 50,000. In fact, it is precisely four times the number of dimensions in this embedding space. This is a design choice; one could opt for a greater or lesser number, but maintaining a clean multiple tends to be advantageous for hardware efficiency.',\n",
       " 'Since this matrix filled with weights maps us into a higher-dimensional space, I will refer to it as W. I will continue to label the vector we are processing as E, and we will denote this bias vector as B, incorporating all of this into the diagram. At this point, a challenge arises: this operation is purely linear, whereas language is inherently a nonlinear process. If the entry we are measuring is high for \"Michael\" plus \"Jordan,\" it would also necessarily be somewhat influenced by \"Michael Phelps\" and \"Alexis\" plus \"Jordan.\" Despite these being conceptually unrelated, what we truly desire is a straightforward yes or no for the full name.\\n\\nThe next step is to pass this large intermediate vector through a simple nonlinear function. A common choice is one that maps all negative values to zero while leaving all positive values unchanged. Continuing with the deep learning tradition of elaborate nomenclature, this straightforward function is often referred to as the rectified linear unit, or ReLU for short. The graph of this function illustrates its behavior. For our hypothetical example, where the first entry of the intermediate vector is 1 if and only if the full name is \"Michael Jordan\" and zero or negative otherwise, after passing it through the ReLU, we obtain a clean value where all zero and negative values are clipped to zero.\\n\\nThus, this output would be 1 for the full name \"Michael Jordan\" and 0 otherwise. In other words, it directly mimics the behavior of an AND gate. Often, models will employ a slightly modified function called the JLU, which retains the same basic shape but is somewhat smoother. However, for our purposes, it is clearer to focus solely on the ReLU. Additionally, when individuals refer to the neurons of a transformer, they are discussing these values.\\n\\nWhenever you encounter the common neural network diagram featuring a layer of dots connected by lines to the previous layer, as we have seen earlier in this series, it typically conveys the combination of a linear step, a matrix multiplication, followed by a simple term-wise nonlinear function like the ReLU. One would say that this neuron is active whenever this value is positive and inactive if that value is zero. The next step resembles the first: you multiply by a large matrix and add a specific bias term. In this case, the number of dimensions in the output returns to the size of the embedding space.',\n",
       " 'I will refer to this as the down projection matrix. Rather than considering the matrix row by row, it is more advantageous to conceptualize it column by column. One effective way to understand matrix multiplication is to envision taking each column of the matrix, multiplying it by the corresponding term in the vector it is processing, and summing all of those rescaled columns. This approach is preferable because the columns share the same dimension as the embedding space, allowing us to interpret them as directions within that space. For instance, we can assume that the model has learned to represent the first column as the direction associated with basketball.\\n\\nThis implies that when the relevant neuron in the first position is activated, we will add this column to the final result. Conversely, if that neuron is inactive, meaning its value is zero, it will have no effect. Furthermore, this column can encapsulate various other features that the model associates with the full name Michael Jordan. Simultaneously, all other columns in this matrix indicate what will be added to the final result if their corresponding neurons are active.\\n\\nIn addition, if there is a bias, it is added consistently, regardless of the neuron values. One might question its purpose. As with all parameterized objects, it is somewhat challenging to determine its exact role; perhaps it serves some bookkeeping function that the network requires, but it can be set aside for the moment. To streamline our notation, I will denote this large matrix as W down and refer to the bias vector as B down, incorporating these into our diagram. As I mentioned earlier, the final result is added to the vector that entered the block at that position, yielding the final output.\\n\\nFor example, if the incoming vector encodes both the first name Michael and the last name Jordan, the sequence of operations will activate the corresponding gate, thereby adding the basketball direction. Consequently, the output will encapsulate all of these elements together. It is important to note that this process occurs in parallel for each of those vectors. Specifically, considering the parameters of GPT-3, this means that this block contains not merely 50,000 neurons, but rather 50,000 multiplied by the number of tokens in the input.',\n",
       " 'Thus, the entire operation consists of two matrix products, each with a bias added and a simple clipping function in between. Those of you who have watched the earlier videos in this series will recognize this structure as the most fundamental type of neural network that we studied. In that example, it was trained to recognize handwritten digits, situated within the context of a transformer for a large language model. This represents one component of a larger architecture, and any attempt to interpret its precise function is intricately linked to the notion of encoding information into vectors within a high-dimensional embedding space. \\n\\nThat is the core lesson. However, I would like to take a step back and reflect on two distinct matters. The first pertains to a form of bookkeeping, while the second involves a particularly thought-provoking fact about higher dimensions that I only discovered upon delving into transformers. \\n\\nIn the last two chapters, we began counting the total number of parameters in GPT-3 and identifying their locations. Let us quickly conclude this exercise. I previously mentioned that the up projection matrix contains just under 50,000 rows, with each row corresponding to the size of the embedding space, which for GPT-3 is 12,288. Multiplying these figures yields 604 million parameters solely for that matrix. The down projection matrix possesses the same number of parameters, albeit in a transposed configuration. \\n\\nConsequently, together they account for approximately 1.2 billion parameters. The bias vector contributes a few additional parameters, but this constitutes a negligible proportion of the total, so I will not elaborate on it. In GPT-3, this sequence of embedding vectors traverses not one, but 96 distinct multilayer perceptrons (MLPs), resulting in a cumulative total of approximately 116 billion parameters dedicated to all of these blocks. This figure represents around two-thirds of the total parameters in the network. When combined with the parameters from the attention blocks, the embedding, and the unembedding, one indeed arrives at the grand total of 175 billion, as advertised. It is worth noting that there exists another set of parameters associated with the normalization steps that this explanation has overlooked. However, similar to the bias vector, they account for a very trivial proportion of the total. \\n\\nRegarding the second point of reflection, you may be curious whether this central illustrative example we have been examining accurately reflects how facts are stored in actual large language models. It is indeed true that the rows of the initial matrix can be conceptualized as directions within this embedding space. This implies that the activation of each neuron indicates the extent to which a given vector aligns with a specific direction. Furthermore, it is also true that the columns of the second matrix indicate what will be added to the result if that neuron is activated.',\n",
       " 'Both of these observations are purely mathematical facts. However, the evidence suggests that individual neurons rarely represent a single, distinct feature, such as Michael Jordan. There may be a compelling reason for this phenomenon, which relates to a concept currently being explored by researchers in interpretability, known as superposition. This hypothesis may elucidate both the challenges associated with interpreting these models and the surprising scalability they exhibit. \\n\\nThe fundamental idea is that if one operates within an n-dimensional space and seeks to represent various features using directions that are mutually orthogonal, then the maximum number of vectors that can be accommodated is limited to n, the number of dimensions. To a mathematician, this defines the concept of dimension. However, the situation becomes intriguing when one relaxes this constraint slightly and permits some degree of noise, allowing features to be represented by vectors that are not strictly orthogonal but nearly so, perhaps at angles ranging from 89 to 91 degrees apart. In two or three dimensions, this relaxation has minimal impact.\\n\\nThis provides little additional flexibility for fitting more vectors, which makes it all the more counterintuitive that in higher dimensions, the outcome changes dramatically. I can illustrate this concept using a simple Python script that generates a list of 100-dimensional vectors, each initialized randomly. This list will contain 10,000 distinct vectors, which is 100 times the number of dimensions. The accompanying plot displays the distribution of angles between pairs of these vectors. Since they are initialized randomly, the angles can range from 0 to 180 degrees.\\n\\nHowever, one will notice a significant bias toward angles closer to 90 degrees, even among random vectors. Subsequently, I will execute a specific optimization process that iteratively adjusts these vectors to enhance their orthogonality. After numerous iterations, the distribution of angles appears as follows. We must zoom in on this distribution, as all possible angles between pairs of vectors fall within a narrow range of 89 to 91 degrees. Generally, a consequence of the Johnson-Lindenstrauss lemma is that the number of vectors that can be densely packed into a space while remaining nearly orthogonal increases exponentially with the number of dimensions.',\n",
       " 'This is highly significant for large language models, which may benefit from associating independent ideas with nearly perpendicular directions. This implies that it is possible for these models to store far more ideas than there are dimensions in the allotted space. This phenomenon may partially explain why model performance appears to scale so effectively with size. A space with ten times as many dimensions can accommodate significantly more than ten times as many independent ideas. This is pertinent not only to the embedding space where the vectors processed by the model reside but also to the vector comprising neurons within the multilayer perceptron we have just studied.\\n\\nIn the case of models like GPT-3, it may not merely be probing 50,000 features; rather, if it leverages this substantial added capacity by utilizing nearly perpendicular directions within the space, it could be probing many more features of the vector being processed. However, if this were the case, individual features would not be discernible as a single neuron activating; instead, it would manifest as a specific combination of neurons. For those interested in further exploration, a key relevant search term is \"sparse autoencoder,\" which is a tool employed by some researchers in interpretability to extract the true features, even when they are highly superimposed across numerous neurons. I will link to several excellent posts from Anthropic that delve into this topic.\\n\\nAt this juncture, we have not addressed every detail of a transformer, but we have covered the most critical points. The primary topic I intend to discuss in the next chapter is the training process. In brief, the fundamental mechanism of training is backpropagation, which we have examined in a separate context in earlier chapters of this series. However, there is more to consider, such as the specific cost function utilized for language models, the concept of fine-tuning through reinforcement learning with human feedback, and the notion of scaling laws. A quick note for the actively engaged among you: there are several non-machine learning-related videos that I am eager to explore before I proceed to the next chapter. \\n\\nThus, it may take some time, but I assure you that it will come in due course.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rewritten_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL REWRITTEN TRANSCRIPT ===\n",
      "\n",
      "Carosia GPT O1. Ho un corso relativo alla gestione dei dati di ricerca (Research Data Management) e il mio compito consiste nell'implementare, utilizzando MySQL Workbench, un modello concettuale che ho creato a partire dai miei dati, con il tuo supporto precedente. Per cominciare, ti invio il report dello studio da cui ho tratto i dati, che ho elaborato personalmente. Successivamente, ti descriverò nel dettaglio come ho strutturato questo modello concettuale, anche grazie al tuo aiuto.\n",
      "\n",
      "Il modello concettuale che ho sviluppato si basa su due oggetti dati, ossia due dataset di sequenziamento dell'RNA a singola cellula: uno relativo a cellule sane e l'altro a cellule tumorali. Utilizzando il dataset delle cellule sane, ho generato delle predizioni applicate al dataset delle cellule tumorali.\n",
      "\n",
      "La struttura del modello concettuale è la seguente. La prima entità è un'entità principale denominata \"Cellula\", che contiene cinque attributi: Cell ID, Cell Cycle Fraction, Ys Cycling, Donor e Chemistry. Da questa entità principale si diramano due sottoentità: \"Healthy Cell\" e \"Tumor Cell\". \n",
      "\n",
      "Gli attributi che ho menzionato, associati all'entità principale \"Cellula\", sono comuni a entrambe le sottoentità. Tuttavia, ciascuna sottoentità possiede anche attributi specifici. Le due sottoentità sono disgiunte, il che significa che i dati presenti nell'entità principale appartengono esclusivamente a una delle due sottoentità, ossia o \"Healthy Cell\" o \"Tumor Cell\". \n",
      "\n",
      "Per quanto riguarda la sottoentità \"Healthy Cell\", gli attributi specifici includono: Cell ID (dell'entità \"Cellula\"), Batch, Stage, Scampi Clusters e Cytograph Clusters. Per la sottoentità \"Tumor Cell\", gli attributi specifici sono: Tumor Cell ID, Age, Clones, Total UMIs, Sample ID, Clusters e Dissociation.\n",
      "\n",
      "Ciascuna di queste sottoentità è collegata a una nuova entità denominata \"Gene\" attraverso una relazione di tipo \"esprime\". Pertanto, sia \"Healthy Cell\" sia \"Tumor Cell\" sono associate alla stessa entità \"Gene\", che presenta i seguenti attributi: Name, Ensemble ID, Chromosome, Strand, Start, End, Selected, Highly Variable e Mean. La relazione tra \"Healthy Cell\" e \"Gene\" e quella tra \"Tumor Cell\" e \"Gene\" sono entrambe di tipo molti-a-molti. Queste relazioni saranno rappresentate da tabelle di espressione genica nelle cellule, con due tabelle distinte: una per \"Healthy Cell\" e una per \"Tumor Cell\".\n",
      "\n",
      "Infine, la sottoentità \"Tumor Cell\" è collegata, tramite una relazione uno-a-uno, a una tabella o entità denominata \"Predictions\".\n",
      "La tabella \"Predictions\" contiene, per ogni \"Tumor Cell ID,\" diverse predizioni generate attraverso metodi distinti. Una colonna rappresenta i risultati ottenuti con il metodo Mahalanobis Distance, un'altra con il metodo Random Forest, una terza colonna riporta la probabilità massima delle predizioni assegnate dal Random Forest, e un'ulteriore colonna, denominata \"GB Map Predicted,\" contiene i risultati di un altro metodo. Inoltre, dall'entità padre \"Cellula\" si diramano altre due entità, \"PCA\" e \"UMAP.\" La relazione tra \"PCA\" e \"Cellula\" è uno-a-uno, poiché ogni riga della tabella \"Cellula\" ha una corrispondente riga in \"PCA.\" In \"PCA\" sono assegnati 50 principal components per ogni cellula, distribuiti su 50 colonne. A ciò si aggiunge una colonna denominata \"Cell Type,\" che distingue tra cellule sane (\"Healthy\") e tumorali (\"Tumor\"). \n",
      "\n",
      "L'entità \"UMAP\" è strutturata in modo simile a \"PCA\" e anch'essa è collegata a \"Cellula\" tramite una relazione uno-a-uno. Contiene il \"Cell ID,\" il tipo di cellula (Healthy o Tumor) e le due dimensioni di embedding generate da UMAP. \n",
      "\n",
      "Infine, l'entità \"Markers\" è collegata all'entità \"Gene.\" \"Markers\" rappresenta una sottocategoria di \"Gene,\" il che implica che tutti i marker sono presenti in \"Gene,\" ma non tutti i geni sono marker. La relazione tra \"Gene\" e \"Markers\" è denominata \"Is\" ed è uno-a-uno: ogni gene può corrispondere a un solo marker, mentre un marker può essere associato a zero o un gene.\n",
      "\n",
      "Per implementare questi concetti in MySQL Workbench, è necessario seguire una serie di passaggi. Iniziare con la creazione delle tabelle principali, definendo le chiavi primarie per ogni entità. Per \"Cellula,\" utilizzare un attributo univoco come \"Cell ID\" come chiave primaria. Le entità \"Healthy Cell\" e \"Tumor Cell\" devono essere collegate a \"Cellula\" tramite una relazione gerarchica, utilizzando \"Cell ID\" come chiave esterna. \n",
      "\n",
      "Per \"PCA\" e \"UMAP,\" creare tabelle separate con \"Cell ID\" come chiave esterna che si riferisce alla chiave primaria di \"Cellula.\" Assicurarsi che le tabelle includano i rispettivi attributi, come i 50 principal components per \"PCA\" e le due dimensioni di embedding per \"UMAP,\" oltre alla colonna \"Cell Type.\" Se si desidera unificare le tabelle di \"PCA\" e \"UMAP\" per cellule sane e tumorali, è possibile farlo direttamente in MySQL Workbench utilizzando una query di unione o, in alternativa, consolidare i dati in un'unica tabella prima dell'importazione.\n",
      "\n",
      "Per \"Predictions,\" definire \"Tumor Cell ID\" come chiave esterna che si riferisce alla chiave primaria di \"Tumor Cell.\" Ogni colonna della tabella deve rappresentare un attributo specifico delle predizioni, come i risultati dei metodi Mahalanobis Distance, Random Forest, e GB Map Predicted.\n",
      "\n",
      "Infine, per \"Gene\" e \"Markers,\" definire \"Gene ID\" come chiave primaria in \"Gene\" e utilizzarlo come chiave esterna in \"Markers.\" La relazione uno-a-uno può essere implementata attraverso vincoli di unicità sulla chiave esterna in \"Markers.\" \n",
      "\n",
      "Si consiglia di iniziare con la creazione della tabella \"Cellula\" e delle sue entità figlie, seguita dalle tabelle \"PCA\" e \"UMAP,\" e infine dalle tabelle \"Predictions,\" \"Gene,\" e \"Markers.\" Prestare attenzione alla definizione delle chiavi esterne e dei vincoli di integrità referenziale per garantire la coerenza del modello.\n",
      "Final transcript saved to final_transcript.txt\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 4) Output the final revised text and running summary as a JSON file\n",
    "final_text = \"\\n\".join(final_rewritten_text)\n",
    "\n",
    "data = {\n",
    "    \"final_text\": final_text,\n",
    "    \"running_summary\": running_summary\n",
    "}\n",
    "\n",
    "output_filename = f\"transcript_{job_name}.json\"\n",
    "try:\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"Final transcript and running summary saved to {output_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {str(e)}\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
