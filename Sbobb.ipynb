{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sbobb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yt_dlp\n",
    "import assemblyai as aai\n",
    "import whisperx\n",
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATION\n",
    "###############################################################################\n",
    "\n",
    "# Define the job name (for output file)\n",
    "job_name = input(\"Job name: \")\n",
    "\n",
    "transcription_service='whisperx'\n",
    "\n",
    "# Load environment variables from config.env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./config.env')\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "ASSEMBLYAI_API_KEY = os.getenv(\"ASSEMBLYAI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate API keys based on selected transcription service\n",
    "# transcription_service = input(\"Choose transcription service (whisper/assemblyai): \").lower()\n",
    "\n",
    "if transcription_service not in ['whisper','whisperx', 'assemblyai']:\n",
    "    raise ValueError(\"Invalid transcription service. Choose 'whisper' or 'assemblyai'\")\n",
    "\n",
    "if transcription_service == 'assemblyai' and not ASSEMBLYAI_API_KEY:\n",
    "    raise ValueError(\"ASSEMBLYAI_API_KEY not found in config.env\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in config.env\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Define OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-4.1-mini\"  # Ensure this model is accessible with your API key\n",
    "\n",
    "# Define chunking parameters\n",
    "CHUNK_WORD_TARGET = 500  # Target words per chunk\n",
    "MAX_SUMMARY_WORDS = 300  # Maximum words in running summary before summarization\n",
    "ENABLE_SUMMARY_SUMMARIZATION = True  # Toggle for summary summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2. ASSEMBLYAI TRANSCRIPTION\n",
    "###############################################################################\n",
    "\n",
    "def download_youtube_audio(youtube_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads the best available audio from a YouTube video, converts it to MP3 (192 kbps),\n",
    "    and saves it into ./audio/ with a proper file name.\n",
    "    \n",
    "    Returns the final path to the downloaded MP3 file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = './audio'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # We'll store the final file in ./audio/<title>.mp3\n",
    "    outtmpl = os.path.join(output_dir, \"%(title)s.%(ext)s\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"format\": \"bestaudio/best\",             # Download best-quality audio\n",
    "        \"outtmpl\": outtmpl,                    # Save to ./audio/<title>.<ext>\n",
    "        \"postprocessors\": [{\n",
    "            \"key\": \"FFmpegExtractAudio\",\n",
    "            \"preferredcodec\": \"mp3\",\n",
    "            \"preferredquality\": \"192\",         # ~192 kbps MP3\n",
    "        }],\n",
    "        \"cookiesfrombrowser\": (\"firefox\",),    # Use Firefox cookies if needed\n",
    "        \"quiet\": True,                         # Suppress non-error messages\n",
    "        \"no_warnings\": True\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=True)\n",
    "        # Prepare the base filename (e.g. 'My Song.m4a' before post-processing)\n",
    "        temp_path = ydl.prepare_filename(info)\n",
    "\n",
    "    # Because of the FFmpegExtractAudio postprocessor, the final file is .mp3\n",
    "    # We just replace the extension if needed:\n",
    "    base, _ = os.path.splitext(temp_path)\n",
    "    final_path = base + \".mp3\"\n",
    "\n",
    "    return final_path\n",
    "        \n",
    "def transcribe_audio_assemblyai(audio_url_or_path: str, language_code: str = \"en\", model: str='nano') -> aai.Transcript:\n",
    "    \"\"\"\n",
    "    Transcribe the audio from a local file path using AssemblyAI.\n",
    "    Returns the transcript object.\n",
    "    \"\"\"\n",
    "    # Set up AssemblyAI\n",
    "    aai.settings.api_key = ASSEMBLYAI_API_KEY\n",
    "    if model == 'nano':\n",
    "        config = aai.TranscriptionConfig(language_code=language_code, speech_model=aai.SpeechModel.nano)\n",
    "    elif model=='slam':\n",
    "        config = aai.TranscriptionConfig(language_code=language_code, speech_model=aai.SpeechModel.slam_1)\n",
    "    \n",
    "    transcriber = aai.Transcriber(config=config)\n",
    "\n",
    "    print(\"Uploading file to AssemblyAI for transcription...\")\n",
    "    transcript = transcriber.transcribe(audio_url_or_path)\n",
    "\n",
    "    # Poll for completion\n",
    "    while transcript.status not in ['completed', 'error']:\n",
    "        print(f\"Transcription status: {transcript.status}. Waiting...\")\n",
    "        time.sleep(5)\n",
    "        transcript = transcriber.get_transcription(transcript.id)\n",
    "\n",
    "    if transcript.status == aai.TranscriptStatus.error:\n",
    "        raise RuntimeError(f\"Transcription failed: {transcript.error}\")\n",
    "\n",
    "    return transcript\n",
    "\n",
    "def transcribe_audio(audio_source: str, language_code: str = \"en\", service: str = \"whisper\", model: str = 'small', batch_size: int = 1, device: str = 'cpu'):\n",
    "    \"\"\"\n",
    "    Unified transcription function that handles both Whisper and AssemblyAI.\n",
    "    Returns transcript text for consistency.\n",
    "    \"\"\"\n",
    "    # Handle YouTube URLs by downloading first\n",
    "    if \"youtube.com\" in audio_source or \"youtu.be\" in audio_source:\n",
    "        print(\"Detected YouTube URL. Downloading audio locally...\")\n",
    "        audio_source = download_youtube_audio(audio_source)\n",
    "        print(f\"Local file path: {audio_source}\")\n",
    "    \n",
    "    if service == \"whisper\":\n",
    "        return transcribe_audio_whisper(audio_source, language_code, model, batch_size, device)\n",
    "    elif service == \"assemblyai\":\n",
    "        transcript_obj = transcribe_audio_assemblyai(audio_source, language_code, model)\n",
    "        return transcript_obj.text\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown transcription service: {service}\")\n",
    "\n",
    "###############################################################################\n",
    "# 3. CHUNKING THE TRANSCRIPT\n",
    "###############################################################################\n",
    "\n",
    "def chunk_text_by_sentences(transcript_text: str, chunk_word_target: int = 600) -> list:\n",
    "    \"\"\"\n",
    "    Splits the transcript text into chunks based on sentences, aiming for about\n",
    "    `chunk_word_target` words each. This works for plain text from Whisper.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Split into sentences using basic punctuation\n",
    "    sentences = re.split(r'[.!?]+', transcript_text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_word_count = len(sentence.split())\n",
    "\n",
    "        # If adding this sentence exceeds the target and current chunk is not empty\n",
    "        if (current_word_count + sentence_word_count) > chunk_word_target and current_chunk:\n",
    "            chunk = \". \".join(current_chunk) + \".\"\n",
    "            chunks.append(chunk)\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_word_count += sentence_word_count\n",
    "\n",
    "    # Add any remaining sentences as the last chunk\n",
    "    if current_chunk:\n",
    "        chunk = \". \".join(current_chunk) + \".\"\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_text_by_paragraphs(transcript: aai.Transcript, chunk_word_target: int = 600) -> list:\n",
    "    \"\"\"\n",
    "    Splits the AssemblyAI transcript into chunks based on its paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = transcript.get_paragraphs()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_text = paragraph.text.strip()\n",
    "        if not paragraph_text:\n",
    "            continue\n",
    "\n",
    "        paragraph_word_count = len(paragraph_text.split())\n",
    "\n",
    "        if (current_word_count + paragraph_word_count) > chunk_word_target and current_chunk:\n",
    "            chunk = \"\\n\".join(current_chunk)\n",
    "            chunks.append(chunk)\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "\n",
    "        current_chunk.append(paragraph_text)\n",
    "        current_word_count += paragraph_word_count\n",
    "\n",
    "    if current_chunk:\n",
    "        chunk = \"\\n\".join(current_chunk)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "###############################################################################\n",
    "# 4. OPENAI REWRITING (PAGE-BY-PAGE)\n",
    "###############################################################################\n",
    "\n",
    "def rewrite_chunk_with_openai(chunk_text: str,\n",
    "                              model: str = OPENAI_MODEL,\n",
    "                              prev_summary: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Sends a chunk of text to OpenAI for rewriting in a 'professorial' register.\n",
    "\n",
    "    Optionally includes `prev_summary` - a short summary of all previously\n",
    "    processed chunks - as context for better continuity across chunks.\n",
    "\n",
    "    Returns the revised chunk as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build system prompt with instructions\n",
    "    system_prompt = (\n",
    "        \"You are an expert in rewriting transcripts with a professorial register. \"\n",
    "        \"You will receive fragments of an\"\n",
    "        \"audio recording regarding a meeting of me and my supervisor, she explains \"\n",
    "        \"things to me regarding bioinformatics, structural biology, network science, graph neural network, computer science.\"\n",
    "        \" Your role is to correct grammar, punctuation, \"\n",
    "        \"and spelling, fix words that may be misrecognized, remove filler words, \"\n",
    "        \"and elevate the text to an academic standard. Output only the revised \"\n",
    "        \"transcript text in plain text, without titles, markdown, or other formatting. \"\n",
    "        \"Maintain context as if it were in medias res.\"\n",
    "        \"Mantain the language of the text, so if it is english, write it in\"\n",
    "        \"english, if it is italian, write it in italian.\"\n",
    "    )\n",
    "\n",
    "    # Build user prompt with the chunk, plus the short summary of prior chunks\n",
    "    # The summary is for context only; it helps the model keep track of earlier topics.\n",
    "    if prev_summary:\n",
    "        user_prompt = (\n",
    "            f\"Here is a short summary of what has come before:\\n{prev_summary}\\n\\n\"\n",
    "            f\"Now, rewrite the following chunk:\\n\\n{chunk_text}\\n\\n\"\n",
    "            \"Output only the revised text. Do not add extra commentary or formatting.\"\n",
    "        )\n",
    "    else:\n",
    "        user_prompt = (\n",
    "            f\"Now, rewrite the following chunk:\\n\\n{chunk_text}\\n\\n\"\n",
    "            \"Output only the revised text. Do not add extra commentary or formatting.\"\n",
    "        )\n",
    "\n",
    "    # Call OpenAI ChatCompletion using the client\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,  # Keep temperature low for consistent rewriting\n",
    "        max_tokens=1500,   # Enough tokens to handle rewriting a ~600-word chunk\n",
    "    )\n",
    "\n",
    "    revised_text = response.choices[0].message.content\n",
    "    return revised_text.strip()\n",
    "\n",
    "def summarize_text_with_openai(text: str,\n",
    "                               model: str = \"chatgpt-4o-mini\") -> str:\n",
    "    \"\"\"\n",
    "    Summarizes the given text in a couple of sentences to maintain context\n",
    "    for future rewriting chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a concise and precise summarizer. Summarize the following text \"\n",
    "        \"in one sentence, focusing on the key ideas. Keep it short. Do not referes \"\n",
    "        \"to the text itself, just provide a single sentence that capture the kay ideas.\"\n",
    "        \"Mantain the language of the text, so if it is english, write it in\"\n",
    "        \"english, if it is italian, write it in italian.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"Text to summarize:\\n{text}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=200,\n",
    "    )\n",
    "\n",
    "    summary = response.choices[0].message.content\n",
    "    return summary.strip()\n",
    "\n",
    "def get_word_count(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the word count of the given text.\n",
    "    \"\"\"\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "# Add these preprocessing functions\n",
    "def get_audio_duration(audio_file_path: str) -> float:\n",
    "    \"\"\"Get the duration of an audio file in seconds.\"\"\"\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(audio_file_path)\n",
    "        return len(audio) / 1000.0  # Convert milliseconds to seconds\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Could not get audio duration: {e}\")\n",
    "\n",
    "def validate_audio_file(audio_file_path: str, max_duration: int = 7200, min_duration: int = 1) -> tuple:\n",
    "    \"\"\"Validate audio file format and duration. Returns (is_valid, message)\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    audio_path = Path(audio_file_path)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not audio_path.exists():\n",
    "        return False, f\"File not found: {audio_path}\"\n",
    "    \n",
    "    # Check file extension\n",
    "    supported_formats = ['.mp3', '.wav', '.m4a', '.flac', '.ogg', '.mp4', '.avi', '.mov']\n",
    "    if audio_path.suffix.lower() not in supported_formats:\n",
    "        return False, f\"Unsupported format: {audio_path.suffix}\"\n",
    "    \n",
    "    # Check duration\n",
    "    try:\n",
    "        duration = get_audio_duration(audio_file_path)\n",
    "        \n",
    "        if duration < min_duration:\n",
    "            return False, f\"Audio too short: {duration:.1f}s (minimum: {min_duration}s)\"\n",
    "        \n",
    "        if duration > max_duration:\n",
    "            return False, f\"Audio too long: {duration:.1f}s (maximum: {max_duration/60:.0f} minutes)\"\n",
    "        \n",
    "        return True, f\"Valid audio file: {duration:.1f}s ({duration/60:.1f} minutes)\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Error validating audio: {e}\"\n",
    "\n",
    "def normalize_audio_for_whisper(input_path: str) -> str:\n",
    "    \"\"\"Convert audio to optimal format for Whisper (16kHz, mono, WAV)\"\"\"\n",
    "    try:\n",
    "        # Create temporary file for normalized audio\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\n",
    "        temp_path = temp_file.name\n",
    "        temp_file.close()\n",
    "        \n",
    "        print(\"Normalizing audio (16kHz, mono, WAV)...\")\n",
    "        \n",
    "        # Try using ffmpeg first (more reliable)\n",
    "        try:\n",
    "            cmd = [\n",
    "                'ffmpeg', '-y', '-i', str(input_path),\n",
    "                '-acodec', 'pcm_s16le',\n",
    "                '-ar', '16000',\n",
    "                '-ac', '1',\n",
    "                '-avoid_negative_ts', 'make_zero',\n",
    "                '-loglevel', 'error',  # Suppress ffmpeg output\n",
    "                temp_path\n",
    "            ]\n",
    "            \n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                raise Exception(f\"FFmpeg error: {result.stderr}\")\n",
    "            \n",
    "            print(\"✓ Audio normalized with FFmpeg\")\n",
    "            return temp_path\n",
    "            \n",
    "        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:\n",
    "            # Fallback to pydub if ffmpeg fails\n",
    "            print(\"FFmpeg not available, using pydub for normalization...\")\n",
    "            \n",
    "            # Clean up failed ffmpeg attempt\n",
    "            if os.path.exists(temp_path):\n",
    "                os.unlink(temp_path)\n",
    "            \n",
    "            # Create new temp file\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\n",
    "            temp_path = temp_file.name\n",
    "            temp_file.close()\n",
    "            \n",
    "            # Load and convert with pydub\n",
    "            audio = AudioSegment.from_file(input_path)\n",
    "            \n",
    "            # Convert to mono and set sample rate\n",
    "            audio = audio.set_channels(1)  # Mono\n",
    "            audio = audio.set_frame_rate(16000)  # 16kHz\n",
    "            \n",
    "            # Export as WAV\n",
    "            audio.export(temp_path, format=\"wav\")\n",
    "            \n",
    "            print(\"✓ Audio normalized with pydub\")\n",
    "            return temp_path\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Clean up on error\n",
    "        if 'temp_path' in locals() and os.path.exists(temp_path):\n",
    "            try:\n",
    "                os.unlink(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "        raise Exception(f\"Audio normalization failed: {e}\")\n",
    "\n",
    "# Replace the transcribe_audio_whisper function with this enhanced version:\n",
    "def transcribe_audio_whisper(audio_file_path: str, language_code: str = 'en', whisper_model: str = 'small', batch_size: int=1, beam_size: int=2, device: str = 'cpu') -> str:\n",
    "    \"\"\"\n",
    "    Transcribe audio file using local Whisper with preprocessing.\n",
    "    Returns the transcript text as a string.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {os.path.basename(audio_file_path)}\")\n",
    "    \n",
    "    # Step 1: Validate audio file\n",
    "    print(\"Validating audio file...\")\n",
    "    # is_valid, message = validate_audio_file(audio_file_path)\n",
    "    is_valid = True\n",
    "    message = ''\n",
    "    if not is_valid:\n",
    "        raise RuntimeError(f\"Audio validation failed: {message}\")\n",
    "    print(f\"✓ {message}\")\n",
    "    \n",
    "    # Step 2: Normalize audio for optimal Whisper performance\n",
    "    normalized_path = None\n",
    "    try:\n",
    "        normalized_path = normalize_audio_for_whisper(audio_file_path)\n",
    "        \n",
    "        print(\"Loading local Whisper model...\")\n",
    "\n",
    "        # Load the whisper model (you can change 'base' to 'small', 'medium', 'large' for better accuracy)\n",
    "        model = WhisperModel(whisper_model, device=device, compute_type=\"int8\")\n",
    "        model_batch = BatchedInferencePipeline(model=model)\n",
    "        \n",
    "        print(\"Transcribing with local Whisper...\")\n",
    "        segments, info = model_batch.transcribe(normalized_path, \n",
    "                                                batch_size=batch_size, \n",
    "                                                # beam_size=beam_size,\n",
    "                                                language=language_code, \n",
    "                                                log_progress=True, \n",
    "                                                word_timestamps=False)\n",
    "        \n",
    "        segments = list(segments)  # The transcription will actually run here.\n",
    "        result = [segment.text.strip() for segment in segments]\n",
    "        \n",
    "        print(\"✓ Transcription completed successfully\")\n",
    "        return ' '.join(result).strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Whisper transcription failed: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary normalized file\n",
    "        if normalized_path and os.path.exists(normalized_path):\n",
    "            try:\n",
    "                os.unlink(normalized_path)\n",
    "                print(\"✓ Cleaned up temporary files\")\n",
    "            except:\n",
    "                pass  # Ignore cleanup errors\n",
    "\n",
    "# Replace the transcribe_audio_whisper function with this enhanced version:\n",
    "def transcribe_audio_whisperX(audio_file_path: str, language_code: str = 'en', whisper_model: str = 'small', batch_size: int=1, beam_size: int = 1, device: str = 'cpu', compute_type:str='int8') -> str:\n",
    "    \"\"\"\n",
    "    Transcribe audio file using local Whisper with preprocessing.\n",
    "    Returns the transcript text as a string.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {os.path.basename(audio_file_path)}\")\n",
    "    \n",
    "    # Step 1: Validate audio file\n",
    "    print(\"Validating audio file...\")\n",
    "    # is_valid, message = validate_audio_file(audio_file_path)\n",
    "    is_valid = True\n",
    "    message = ''\n",
    "    if not is_valid:\n",
    "        raise RuntimeError(f\"Audio validation failed: {message}\")\n",
    "    print(f\"✓ {message}\")\n",
    "    \n",
    "    # Step 2: Normalize audio for optimal Whisper performance\n",
    "    normalized_path = None\n",
    "    try:\n",
    "        normalized_path = normalize_audio_for_whisper(audio_file_path)\n",
    "        \n",
    "        print(\"Loading local Whisper model...\")\n",
    "\n",
    "        # Load the whisper model (you can change 'base' to 'small', 'medium', 'large' for better accuracy)\n",
    "        model = whisperx.load_model(whisper_model, device, compute_type=compute_type, )\n",
    "        audio = whisperx.load_audio(normalized_path)\n",
    "        segments, lang = model.transcribe(audio, \n",
    "                                          batch_size=batch_size, \n",
    "                                          language=language_code, \n",
    "                                          # beam_size=beam_size, \n",
    "                                          print_progress=True).values()\n",
    "        \n",
    "        text = [segment['text'] for segment in segments]   \n",
    "        print(\"✓ Transcription completed successfully\")\n",
    "        return ' '.join(text).strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Whisper transcription failed: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary normalized file\n",
    "        if normalized_path and os.path.exists(normalized_path):\n",
    "            try:\n",
    "                os.unlink(normalized_path)\n",
    "                print(\"✓ Cleaned up temporary files\")\n",
    "            except:\n",
    "                pass  # Ignore cleanup errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transcribe_audio(audio_source: str, language_code: str = \"en\", service: str = \"whisper\", model: str = 'small', batch_size: int = 1, beam_size: int = 1, device: str = 'cpu', compute_type:str = 'int8'):\n",
    "    \"\"\"\n",
    "    Unified transcription function that handles both Whisper and AssemblyAI.\n",
    "    Returns transcript text for consistency.\n",
    "    \"\"\"\n",
    "    # Handle YouTube URLs by downloading first\n",
    "    if \"youtube.com\" in audio_source or \"youtu.be\" in audio_source:\n",
    "        print(\"Detected YouTube URL. Downloading audio locally...\")\n",
    "        audio_source = download_youtube_audio(audio_source)\n",
    "        print(f\"Local file path: {audio_source}\")\n",
    "    \n",
    "    if service == \"whisper\":\n",
    "        return transcribe_audio_whisper(audio_source, language_code, model, batch_size, device, compute_type)\n",
    "    if service == \"whisperx\":\n",
    "        return transcribe_audio_whisperX(audio_source, language_code, model, batch_size, beam_size, device, compute_type)\n",
    "    elif service == \"assemblyai\":\n",
    "        transcript_obj = transcribe_audio_assemblyai(audio_source, language_code, model)\n",
    "        return transcript_obj.text\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown transcription service: {service}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_source = input('Path to audio file: ')\n",
    "audio_source = r\"C:\\Users\\nicco\\Downloads\\storia_romana_4.2.mp3\"\n",
    "\n",
    "print(\"Transcribing audio... please wait.\")\n",
    "\n",
    "# Medium with batch_size=4 takes around 2 minutes to process a 10 minutes record on a i7 cpu with 16GB on laptop, \n",
    "# use GPU or small model for faster processing (small takes around 3 minutes but the results are significantly worse for techincal words).\n",
    "# You could also try larger batch sizes in large RAM/VRAM available, consider that medium with int8 quantization take aroung 1GB of Ram on its own.\n",
    "# Using int8 instead of float32 reduce the time by ~30% and a 4 times smaller model in memory, at the cost of some precison in the translation. \n",
    "# Batch sizes larger than 4 have only marginal increases on time (at least on my laptop), and cames at larger memory usage.\n",
    "# Medium is 2.7x slower than Small, Turbo is 3.3x slower, distill-large 3.1x, large-v3 is 5x\n",
    "# batch_size and device only inlfuence whisper. \n",
    "\n",
    "full_transcript_text = transcribe_audio(audio_source,  language_code='it', service=transcription_service, model = 'large-v3-turbo', batch_size=6, beam_size=6, device = 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting transcript into chunks...\")\n",
    "\n",
    "match transcription_service:\n",
    "    case 'assemblyai':\n",
    "        chunks = chunk_text_by_paragraphs(full_transcript_text, chunk_word_target=CHUNK_WORD_TARGET)\n",
    "    case 'whisper':\n",
    "        chunks = chunk_text_by_sentences(full_transcript_text, chunk_word_target=CHUNK_WORD_TARGET)\n",
    "    case 'whisperx':\n",
    "        chunks = chunk_text_by_sentences(full_transcript_text, chunk_word_target=CHUNK_WORD_TARGET)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunk(s) of ~{CHUNK_WORD_TARGET} words each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) For each chunk, rewrite with OpenAI\n",
    "final_rewritten_text = []\n",
    "running_summary = \"\"  # Will accumulate short summaries of prior chunks\n",
    "print(\"Rewriting the transcript ...\")\n",
    "\n",
    "for i, chunk_text in enumerate(chunks, start=1):\n",
    "    # print(f\"Rewriting chunk {i}/{len(chunks)}...\")\n",
    "\n",
    "    # Rewrite the chunk\n",
    "    try:\n",
    "        revised_text = rewrite_chunk_with_openai(\n",
    "            chunk_text=chunk_text,\n",
    "            model=OPENAI_MODEL,\n",
    "            prev_summary=running_summary\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error rewriting chunk {i}: {str(e)}\")\n",
    "        continue  # Skip to the next chunk\n",
    "\n",
    "    # Append the revised text to our final output\n",
    "    final_rewritten_text.append(revised_text)\n",
    "\n",
    "    # Summarize this revised chunk to update context\n",
    "    try:\n",
    "        chunk_summary = summarize_text_with_openai(revised_text, model=OPENAI_MODEL)\n",
    "        # print(f\"Summary for chunk {i}: {chunk_summary}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error summarizing chunk {i}: {str(e)}\")\n",
    "        chunk_summary = \"\"\n",
    "\n",
    "    # Append new summary to the running summary\n",
    "    \n",
    "    if ENABLE_SUMMARY_SUMMARIZATION:\n",
    "        running_summary += f\" {chunk_summary}\"\n",
    "        # Check if running_summary exceeds MAX_SUMMARY_WORDS\n",
    "        if get_word_count(running_summary) > MAX_SUMMARY_WORDS:\n",
    "            # print(\"Running summary exceeds maximum word limit. Summarizing the running summary...\")\n",
    "            try:\n",
    "                summarized_running_summary = summarize_text_with_openai(running_summary, model=OPENAI_MODEL)\n",
    "                running_summary = summarized_running_summary\n",
    "                # print(f\"Summarized running summary: {running_summary}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error summarizing running summary: {str(e)}\")\n",
    "                # Optionally, you can reset the running_summary or keep it as is\n",
    "    else:\n",
    "        running_summary += f\" {chunk_summary}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# 4) Output the final revised text and running summary as a JSON file\n",
    "final_text = \" \".join(final_rewritten_text)\n",
    "\n",
    "# Correzione delle sequenze di escape letterali nel testo\n",
    "final_text = re.sub(r'\\\\n', '\\n', final_text)  # Converti \\n in veri a capo\n",
    "final_text = re.sub(r\"\\\\'\", \"'\", final_text)   # Converti \\' in apostrofi normali\n",
    "final_text = re.sub(r'\\\\\"', '\"', final_text)   # Converti \\\" in virgolette normali\n",
    "final_text = re.sub(r'\\\\t', '\\t', final_text)  # Converti \\t in tabulazioni\n",
    "\n",
    "data = {\n",
    "    \"final_text\": final_text,\n",
    "    \"running_summary\": running_summary,\n",
    "    \"audio_transcript\": \" \".join(chunks)\n",
    "}\n",
    "\n",
    "output_filename = f\"transcript_{job_name}.json\"\n",
    "try:\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)  # ensure_ascii=False preserva i caratteri non-ASCII\n",
    "    print(f\"Final transcript and running summary saved to {output_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {str(e)}\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daily_notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
