{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1. CONFIGURATION\n",
    "###############################################################################\n",
    "\n",
    "# Define the job name (for output file)\n",
    "job_name = input(\"Job name: \")\n",
    "\n",
    "# Load environment variables from config.env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./config.env')\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "ASSEMBLYAI_API_KEY = os.getenv(\"ASSEMBLYAI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate API keys\n",
    "if not ASSEMBLYAI_API_KEY:\n",
    "    raise ValueError(\"ASSEMBLYAI_API_KEY not found in config.env\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in config.env\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "# client = OpenAI(api_key=\"<DeepSeek API Key>\", base_url=\"https://api.deepseek.com\")from dotenv import load_dotenv\n",
    "\n",
    "# Define OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"  # Ensure this model is accessible with your API key\n",
    "\n",
    "# Define chunking parameters\n",
    "CHUNK_WORD_TARGET = 500  # Target words per chunk\n",
    "MAX_SUMMARY_WORDS = 300  # Maximum words in running summary before summarization\n",
    "ENABLE_SUMMARY_SUMMARIZATION = True  # Toggle for summary summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yt_dlp\n",
    "import assemblyai as aai\n",
    "\n",
    "###############################################################################\n",
    "# 2. ASSEMBLYAI TRANSCRIPTION\n",
    "###############################################################################\n",
    "\n",
    "def extract_audio_from_youtube(youtube_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the best available audio URL (m4a format) from the given YouTube URL.\n",
    "    \"\"\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"cookiesfrombrowser\": ('firefox',)\n",
    "    }\n",
    "    \n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=True)\n",
    "\n",
    "    # Iterate over formats in reverse (best quality first) and pick one with audio only\n",
    "    for fmt in reversed(info.get(\"formats\", [])):\n",
    "        if fmt.get(\"acodec\") != \"none\" and fmt.get(\"ext\") == \"m4a\":\n",
    "            return fmt[\"url\"]\n",
    "\n",
    "def transcribe_audio_assemblyai(audio_url_or_path: str, language_code: str = \"en\") -> aai.Transcript:\n",
    "    \"\"\"\n",
    "    Transcribe the audio from the given URL or local file path using AssemblyAI.\n",
    "    If a YouTube URL is provided, it automatically extracts the audio URL.\n",
    "    Returns the transcript object.\n",
    "    \"\"\"\n",
    "    # If the input appears to be a YouTube URL, extract the audio URL.\n",
    "    if \"youtube.com\" in audio_url_or_path or \"youtu.be\" in audio_url_or_path:\n",
    "        print(\"YouTube URL detected. Extracting audio URL...\")\n",
    "        audio_url_or_path = extract_audio_from_youtube(audio_url_or_path)\n",
    "        print(f\"YouTube video audio URL: {audio_url_or_path}\")\n",
    "    \n",
    "    # Set up AssemblyAI\n",
    "    aai.settings.api_key = ASSEMBLYAI_API_KEY\n",
    "    config = aai.TranscriptionConfig(language_code=language_code)\n",
    "    transcriber = aai.Transcriber(config=config)\n",
    "\n",
    "    # Start transcription\n",
    "    transcript = transcriber.transcribe(audio_url_or_path)\n",
    "\n",
    "    # Poll for completion\n",
    "    while transcript.status not in ['completed', 'error']:\n",
    "        print(f\"Transcription status: {transcript.status}. Waiting...\")\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "        transcript = transcriber.get_transcription(transcript.id)\n",
    "\n",
    "    # Check for errors\n",
    "    if transcript.status == aai.TranscriptStatus.error:\n",
    "        raise RuntimeError(f\"Transcription failed: {transcript.error}\")\n",
    "\n",
    "    return transcript\n",
    "\n",
    "###############################################################################\n",
    "# 3. CHUNKING THE TRANSCRIPT\n",
    "###############################################################################\n",
    "\n",
    "def chunk_text_by_paragraphs(transcript: aai.Transcript, chunk_word_target: int = 600) -> list:\n",
    "    \"\"\"\n",
    "    Splits the transcript into chunks based on its paragraphs, aiming for about\n",
    "    `chunk_word_target` words each. Accumulates paragraphs until the target is reached.\n",
    "    \n",
    "    Returns a list of textual chunks (strings).\n",
    "    \"\"\"\n",
    "    paragraphs = transcript.get_paragraphs()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_text = paragraph.text.strip()\n",
    "        if not paragraph_text:\n",
    "            continue  # Skip empty paragraphs\n",
    "\n",
    "        paragraph_word_count = len(paragraph_text.split())\n",
    "\n",
    "        # If adding this paragraph exceeds the target and current chunk is not empty, create a new chunk\n",
    "        if (current_word_count + paragraph_word_count) > chunk_word_target and current_chunk:\n",
    "            chunk = \"\\n\".join(current_chunk)\n",
    "            chunks.append(chunk)\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "\n",
    "        # Add the paragraph to the current chunk\n",
    "        current_chunk.append(paragraph_text)\n",
    "        current_word_count += paragraph_word_count\n",
    "\n",
    "    # Add any remaining paragraphs as the last chunk\n",
    "    if current_chunk:\n",
    "        chunk = \"\\n\".join(current_chunk)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "###############################################################################\n",
    "# 4. OPENAI REWRITING (PAGE-BY-PAGE)\n",
    "###############################################################################\n",
    "def rewrite_chunk_with_openai(chunk_text: str,\n",
    "                              model: str = OPENAI_MODEL,\n",
    "                              prev_summary: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Sends a chunk of text to OpenAI for rewriting in a 'professorial' register.\n",
    "\n",
    "    Optionally includes `prev_summary` – a short summary of all previously\n",
    "    processed chunks – as context for better continuity across chunks.\n",
    "\n",
    "    Returns the revised chunk as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build system prompt with instructions\n",
    "    system_prompt = (\n",
    "        \"You are an expert in rewriting transcripts with a professorial register. \"\n",
    "        \"You will receive fragments of a university lesson transcript generated \"\n",
    "        \"from an audio recording. Your role is to correct grammar, punctuation, \"\n",
    "        \"and spelling, fix words that may be misrecognized, remove filler words, \"\n",
    "        \"and elevate the text to an academic standard. Output only the revised \"\n",
    "        \"transcript text in plain text, without titles, markdown, or other formatting. \"\n",
    "        \"Maintain context as if it were in medias res.\"\n",
    "    )\n",
    "\n",
    "    # Build user prompt with the chunk, plus the short summary of prior chunks\n",
    "    # The summary is for context only; it helps the model keep track of earlier topics.\n",
    "    if prev_summary:\n",
    "        user_prompt = (\n",
    "            f\"Here is a short summary of what has come before:\\n{prev_summary}\\n\\n\"\n",
    "            f\"Now, rewrite the following chunk:\\n\\n{chunk_text}\\n\\n\"\n",
    "            \"Output only the revised text. Do not add extra commentary or formatting.\"\n",
    "        )\n",
    "    else:\n",
    "        user_prompt = (\n",
    "            f\"Now, rewrite the following chunk:\\n\\n{chunk_text}\\n\\n\"\n",
    "            \"Output only the revised text. Do not add extra commentary or formatting.\"\n",
    "        )\n",
    "\n",
    "    # Call OpenAI ChatCompletion using the client\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,  # Keep temperature low for consistent rewriting\n",
    "        max_tokens=1500,   # Enough tokens to handle rewriting a ~600-word chunk\n",
    "    )\n",
    "\n",
    "    revised_text = response.choices[0].message.content\n",
    "    return revised_text.strip()\n",
    "\n",
    "def summarize_text_with_openai(text: str,\n",
    "                               model: str = \"chatgpt-4o-mini\") -> str:\n",
    "    \"\"\"\n",
    "    Summarizes the given text in a couple of sentences to maintain context\n",
    "    for future rewriting chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a concise and precise summarizer. Summarize the following text \"\n",
    "        \"in one sentence, focusing on the key ideas. Keep it short. Do not referes \"\n",
    "        \"to the text itself, just provide a single sentence that capture the kay ideas.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"Text to summarize:\\n{text}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=200,\n",
    "    )\n",
    "\n",
    "    summary = response.choices[0].message.content\n",
    "    return summary.strip()\n",
    "\n",
    "def get_word_count(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the word count of the given text.\n",
    "    \"\"\"\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "\n",
    "def download_youtube_audio(youtube_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads the best available audio from a YouTube video, converts it to MP3 (192 kbps),\n",
    "    and saves it into ./audio/ with a proper file name.\n",
    "    \n",
    "    Returns the final path to the downloaded MP3 file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = './audio'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # We'll store the final file in ./audio/<title>.mp3\n",
    "    outtmpl = os.path.join(output_dir, \"%(title)s.%(ext)s\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"format\": \"bestaudio/best\",             # Download best-quality audio\n",
    "        \"outtmpl\": outtmpl,                    # Save to ./audio/<title>.<ext>\n",
    "        \"postprocessors\": [{\n",
    "            \"key\": \"FFmpegExtractAudio\",\n",
    "            \"preferredcodec\": \"mp3\",\n",
    "            \"preferredquality\": \"192\",         # ~192 kbps MP3\n",
    "        }],\n",
    "        \"cookiesfrombrowser\": (\"firefox\",),    # Use Firefox cookies if needed\n",
    "        \"quiet\": True,                         # Suppress non-error messages\n",
    "        \"no_warnings\": True\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=True)\n",
    "        # Prepare the base filename (e.g. 'My Song.m4a' before post-processing)\n",
    "        temp_path = ydl.prepare_filename(info)\n",
    "\n",
    "    # Because of the FFmpegExtractAudio postprocessor, the final file is .mp3\n",
    "    # We just replace the extension if needed:\n",
    "    base, _ = os.path.splitext(temp_path)\n",
    "    final_path = base + \".mp3\"\n",
    "\n",
    "    return final_path\n",
    "\n",
    "def transcribe_audio_assemblyai(audio_url_or_path: str, language_code: str = \"en\") -> aai.Transcript:\n",
    "    \"\"\"\n",
    "    Transcribe the audio from a local file path or (if it's YouTube) download it first,\n",
    "    and then pass the local file to AssemblyAI for transcription.\n",
    "    \"\"\"\n",
    "    # If the input appears to be a YouTube URL, download the audio locally\n",
    "    if \"youtube.com\" in audio_url_or_path or \"youtu.be\" in audio_url_or_path:\n",
    "        print(\"Detected YouTube URL. Downloading audio locally...\")\n",
    "        audio_url_or_path = download_youtube_audio(audio_url_or_path)\n",
    "        print(f\"Local file path: {audio_url_or_path}\")\n",
    "    \n",
    "    # Set up AssemblyAI\n",
    "    aai.settings.api_key = ASSEMBLYAI_API_KEY\n",
    "    config = aai.TranscriptionConfig(language_code=language_code)\n",
    "    transcriber = aai.Transcriber(config=config)\n",
    "\n",
    "    print(\"Uploading file to AssemblyAI for transcription...\")\n",
    "    # The transcriber can accept a local path, in which case it will upload it to AssemblyAI.\n",
    "    transcript = transcriber.transcribe(audio_url_or_path)\n",
    "\n",
    "    # Poll for completion\n",
    "    while transcript.status not in ['completed', 'error']:\n",
    "        print(f\"Transcription status: {transcript.status}. Waiting...\")\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "        transcript = transcriber.get_transcription(transcript.id)\n",
    "\n",
    "    # Check for errors\n",
    "    if transcript.status == aai.TranscriptStatus.error:\n",
    "        raise RuntimeError(f\"Transcription failed: {transcript.error}\")\n",
    "\n",
    "    return transcript\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing audio... please wait.\n",
      "Detected YouTube URL. Downloading audio locally...\n",
      "Local file path: ./audio/Stanford CS229 I Machine Learning I Building Large Language Models (LLMs).mp3\n",
      "Uploading file to AssemblyAI for transcription...\n",
      "Transcription complete.\n"
     ]
    }
   ],
   "source": [
    "# 1) Transcribe audio\n",
    "# You can point to a local file, remote URL or a YouTube video. E.g.:\n",
    "# audio_source = \"https://assembly.ai/path_to_your_audio_file.mp3\"\n",
    "# or\n",
    "# audio_source = \"./local_file.mp3\"\n",
    "# or\n",
    "# audio_source = \"https://www.youtube.com/watch?v=YOUR_VIDEO\"\n",
    "audio_source = input('Path to audio file: ')  # Replace with your audio file path or URL\n",
    "\n",
    "print(\"Transcribing audio... please wait.\")\n",
    "try:\n",
    "    full_transcript_text = transcribe_audio_assemblyai(audio_source, language_code='en')\n",
    "except RuntimeError as e:\n",
    "    print(str(e))\n",
    "\n",
    "print(\"Transcription complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting transcript into chunks based on paragraphs...\n",
      "Created 41 chunk(s) of ~500 words each.\n"
     ]
    }
   ],
   "source": [
    "# 2) Chunk the transcript using paragraphs\n",
    "print(\"Splitting transcript into chunks based on paragraphs...\")\n",
    "chunks = chunk_text_by_paragraphs(full_transcript_text, chunk_word_target=CHUNK_WORD_TARGET)\n",
    "print(f\"Created {len(chunks)} chunk(s) of ~{CHUNK_WORD_TARGET} words each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revriting the transcript ...\n"
     ]
    }
   ],
   "source": [
    "# 3) For each chunk, rewrite with OpenAI\n",
    "final_rewritten_text = []\n",
    "running_summary = \"\"  # Will accumulate short summaries of prior chunks\n",
    "print(\"Revriting the transcript ...\")\n",
    "\n",
    "for i, chunk_text in enumerate(chunks, start=1):\n",
    "    # print(f\"Rewriting chunk {i}/{len(chunks)}...\")\n",
    "\n",
    "    # Rewrite the chunk\n",
    "    try:\n",
    "        revised_text = rewrite_chunk_with_openai(\n",
    "            chunk_text=chunk_text,\n",
    "            model=OPENAI_MODEL,\n",
    "            prev_summary=running_summary\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error rewriting chunk {i}: {str(e)}\")\n",
    "        continue  # Skip to the next chunk\n",
    "\n",
    "    # Append the revised text to our final output\n",
    "    final_rewritten_text.append(revised_text)\n",
    "\n",
    "    # Summarize this revised chunk to update context\n",
    "    try:\n",
    "        chunk_summary = summarize_text_with_openai(revised_text, model=OPENAI_MODEL)\n",
    "        # print(f\"Summary for chunk {i}: {chunk_summary}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error summarizing chunk {i}: {str(e)}\")\n",
    "        chunk_summary = \"\"\n",
    "\n",
    "    # Append new summary to the running summary\n",
    "    \n",
    "    if ENABLE_SUMMARY_SUMMARIZATION:\n",
    "        running_summary += f\" {chunk_summary}\"\n",
    "        # Check if running_summary exceeds MAX_SUMMARY_WORDS\n",
    "        if get_word_count(running_summary) > MAX_SUMMARY_WORDS:\n",
    "            # print(\"Running summary exceeds maximum word limit. Summarizing the running summary...\")\n",
    "            try:\n",
    "                summarized_running_summary = summarize_text_with_openai(running_summary, model=OPENAI_MODEL)\n",
    "                running_summary = summarized_running_summary\n",
    "                # print(f\"Summarized running summary: {running_summary}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error summarizing running summary: {str(e)}\")\n",
    "                # Optionally, you can reset the running_summary or keep it as is\n",
    "    else:\n",
    "        running_summary += f\" {chunk_summary}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let us begin. Today, I will be discussing the construction of large language models (LLMs). Many of you are likely familiar with LLMs, which are essentially the chatbots that have garnered significant attention recently, such as ChatGPT from OpenAI, Claude from Anthropic, Gemini, Llama, and other similar models. \\n\\nOur focus today will be on how these models function. This will be an overview, as we have only one lecture and it is challenging to condense all the information. However, I aim to address the various components necessary for training these LLMs. If you have questions, please feel free to interrupt me. If you have a question, it is likely that others in the room or on Zoom share the same inquiry, so do not hesitate to ask.\\n\\nWhat is crucial when training LLMs? There are several key components to consider. First is the architecture; as you may know, LLMs are a type of neural network. When contemplating neural networks, one must consider the architecture employed. Another significant component is the training loss and the training algorithm, which pertain to the methods used to train these models. Additionally, the data used for training is essential. Evaluation is also critical, as it determines how one assesses progress toward the objectives of LLMs. Lastly, the system component is vital, as it involves the implementation of these models on modern hardware, which is particularly important given the large size of these models. \\n\\nNow more than ever, systems are a crucial topic for LLMs. The five components I have mentioned are fundamental. It is important to note that LLMs are all based on transformers, or at least some variant of transformers. However, I will not delve into the architecture today, primarily because I delivered a lecture on transformers a few weeks ago, and there is an abundance of information available online regarding transformers. In contrast, there is considerably less information on the other four topics, which I wish to emphasize. \\n\\nFurthermore, it is worth mentioning that much of academia tends to concentrate on architecture, training algorithms, and losses. As academics, we often find ourselves preoccupied with developing new architectures and models, believing them to be of paramount importance. However, in practice, what truly matters is predominantly the other three topics.',\n",
       " 'Data evaluation and systems are the primary focus of the industry. This is one of the reasons I will not delve deeply into architecture, as the other components are significantly more important. To provide an overview of the lecture, I will discuss pre-training.\\n\\nPre-training is a term you may have encountered; it refers to the classical language modeling paradigm, where a language model is trained to essentially model the entirety of the Internet. In contrast, post-training is a more recent paradigm that involves taking these large language models and transforming them into AI assistants, a trend that has gained prominence since the advent of ChatGPT.\\n\\nIf you are familiar with GPT-2 or GPT-3, you are looking at the pre-training phase. Conversely, if you have heard of ChatGPT, you are entering the realm of post-training. I will address both phases, beginning with pre-training. Specifically, I will discuss the tasks associated with pre-training large language models and the loss functions employed.\\n\\nTo recap briefly, language models, at a high level, are models of probability distributions over sequences of tokens or words. They represent a model of P(x1 to xl), where x1 is the first word and xl is the last word in the sequence or sentence. For instance, if we consider the sentence \"the mouse ate the cheese,\" the language model provides a probability of this sentence being articulated by a human or being found online. \\n\\nIn contrast, if we examine the sentence \"the the mouse ate cheese,\" which contains grammatical errors, the model should recognize that this has a lower likelihood of appearing in written form. Similarly, for the sentence \"the cheese ate the mouse,\" the model should ideally possess semantic knowledge, understanding that cheese typically does not consume mice, thus rendering this sentence less probable than the first.\\n\\nAt a high level, this encapsulates the essence of language models. A term you may have frequently encountered in recent discussions is \"generative models.\" These are models capable of generating sentences or data. The classification of language models as generative models arises from the fact that once a model of a distribution is established, one can sample from this model to generate data, including sentences.\\n\\nCurrently, the predominant type of models in use are autoregressive language models. The fundamental concept behind autoregressive language models is the decomposition of the distribution over words into a series of conditional probabilities.',\n",
       " 'Into the distribution of the first word, we multiply it by the distribution, or the likelihood, of the second word given the first word, and then multiply it by the probability of the third word given the first two words. There is no approximation here; this is simply the chain rule of probability, which I trust you are all familiar with. This is one method of modeling a distribution.\\n\\nMore concisely, this can be expressed as a product of probabilities of the next word given all preceding context. This approach is what we refer to as autoregressive language models. It is important to note that this is not the only method for modeling distributions; it has its advantages and disadvantages.\\n\\nOne disadvantage of autoregressive language models is that when sampling from such a model, the process essentially involves a loop that generates the next word, conditions on that word, and then generates another word. Consequently, if one wishes to generate a longer sentence, it requires more time to do so. While there are limitations to this current paradigm, it is the framework we are currently utilizing.\\n\\nAt a high level, the task of an autoregressive language model is to predict the next word. For instance, if we have a sentence like \"she likely prefers,\" one potential next word might be \"dogs.\" The process begins with tokenization, where we take these words or subwords and assign an ID to each token. For example, we might have IDs 1, 2, and 3, which are then passed through a model.\\n\\nAs previously mentioned, we will not delve into the architecture; we simply pass the tokens through the model to obtain a probability distribution over the next word or token. We then sample from this distribution to obtain a new token, which we subsequently detokenize to retrieve the new ID. This is the fundamental process for sampling from a language model. It is important to note that the last two steps are only necessary during inference. \\n\\nDuring training, we focus on predicting the most likely token and compare it to the actual token that occurred in practice. We then adjust the weights of the model to increase the probability of generating that token. Thus, we arrive at autoregressive neural language models. To be more specific, without discussing the architecture, the first step involves...',\n",
       " 'On the previous slide, when predicting the probability of the next tokens, does this mean that your final output vector must have the same dimensionality as the number of tokens you possess? Yes. How do you address the situation when you have more tokens, such as when you are adding additional tokens to your corpus? \\n\\nWe will discuss tokenization later, which will provide some clarity on this matter. Essentially, you cannot simply add new tokens; I am somewhat exaggerating. There are methods for doing so, but in practice, it is not commonly implemented. Therefore, it is crucial to consider how you tokenize your text. This is why we will address it later. It is important to note that the vocabulary size, or the number of tokens you have, essentially determines the output of your language model, which can be quite substantial.\\n\\nNow, regarding autoregressive neural language models, the first step is to take every word or token and embed them, resulting in a vector representation for each token. You then pass these representations through a neural network, specifically a transformer, which generates a representation for all the words in the context, effectively representing the entire sentence. This representation is subsequently passed through a linear layer to map it to the number of outputs, which corresponds to the number of tokens.\\n\\nFollowing this, you apply a softmax function, yielding a probability distribution over the next words given every word in the context. The loss function employed is essentially a classification task for the next token, which is a straightforward machine learning task. You utilize cross-entropy loss, where you compare the actual target that occurred—represented as a one-hot encoding. For instance, if the actual word is \"cat,\" the one-hot distribution indicates this.\\n\\nHere, you can observe the distribution generated by your model. The cross-entropy loss increases the probability of generating \"cat\" while decreasing the probabilities of all other tokens. It is important to note that this process is equivalent to maximizing the text log likelihood. You can reformulate the maximization of the probability in this autoregressive language modeling task as minimizing the loss, which is represented by the cross-entropy loss.',\n",
       " \"Minimizing the loss is equivalent to maximizing the likelihood of your text. Are there any questions? \\n\\nNow, let us discuss tokenizers. This is an aspect that is often overlooked, yet tokenizers are of paramount importance. It is essential to understand, at least at a high level, their function. Why do we need tokenizers in the first place? \\n\\nFirst, tokenization is more general than merely identifying words. One might assume that each word could serve as a token. However, if there is a typographical error in a word, there may not be a corresponding token for that misspelled word, which complicates the process of inputting it into a large language model. \\n\\nWhat then is the solution? Furthermore, while tokenization by words may suffice for languages based on the Latin alphabet, it poses challenges for languages such as Thai, where words are not separated by spaces. Thus, tokens must be understood as being more general than individual words. \\n\\nSecondly, one might consider tokenizing every sentence character by character, designating 'A' as one token and 'B' as another. While this approach could function effectively, it results in exceedingly long sequences. As you may recall from the lecture on Transformers, the computational complexity increases quadratically with the length of sequences. \\n\\nTherefore, it is undesirable to have excessively long sequences. Tokenizers aim to address these two issues by assigning a single token to common subsequences. Typically, one should conceptualize that the average token comprises approximately three to four letters. There are numerous algorithms for tokenization; I will discuss one of the most prevalent, known as byte pair encoding. \\n\\nThis method is among the two most common tokenizers. To train a tokenizer, one begins with a substantial corpus of text. At this stage, I am not referring to the training of a large language model; this is solely focused on the tokenization process. Thus, we have a large corpus of text consisting of these five words.\",\n",
       " \"Then, you associate every character in this corpus of text with a different token. Here, I have split each character into a distinct token and color-coded all of those tokens. Subsequently, you examine your text, and each time you encounter pairs of tokens that are particularly common, specifically the most common pair of tokens, you merge them. For instance, you observe the tokens 'T' and 'O' appearing next to each other three times, thus designating this as a new token. You continue this process, resulting in tokens such as 'tok' occurring three times, 'toke' with an 'e' appearing twice, and 'token' also occurring twice, along with 'ex,' which appears twice as well. If you were to train a tokenizer on this relatively small corpus of text, this is how you would conclude with a token from a trained tokenizer. In practice, this process is applied to much larger corpora of text. This is the actual tokenizer of what I believe is GPT-3 or ChatGPT, and here you can see how it separates these words.\\n\\nEssentially, you observe the same principle as in the previous example; 'token' becomes its own token. The tokenizer is actually divided into two tokens: 'token' and 'isa.' That concludes our discussion on tokenizers. Are there any questions regarding this?\\n\\nYes, how do you handle spaces and punctuation? There is indeed a step prior to tokenization, which we refer to as pre-tokenization, directly addressing your inquiry. Theoretically, there is no necessity to treat spaces and punctuation separately. You could assign every space its own token and every punctuation mark its own token, subsequently performing all the merging. However, there is an efficiency consideration; training these tokenizers can be time-consuming. You must evaluate every pair of tokens, so what you ultimately do is state that if there is a space, pre-tokenizers, which are largely English-specific, will not examine the token preceding the space and the token following it. Therefore, you do not merge tokens across spaces. This is merely a computational optimization; theoretically, you could handle it in the same manner as any other character. \\n\\nWhen you merge tokens, do you delete the tokens that you have merged, or do you retain the smaller tokens that were merged? You actually retain the smaller tokens.\",\n",
       " 'In reality, it does not matter much because, typically, in a large corpus of text, you will have a comprehensive representation of the language. However, it is advisable to retain the smaller tokens. The reason for this is that, in cases where there are grammatical mistakes or typographical errors, it is still important to represent these words at the character level. \\n\\nAre the tokens unique? For instance, in this case, does a token have only one occurrence, or do you need to account for multiple occurrences so that they can take on different meanings? I understand your question. No, every token has its own unique identifier. \\n\\nThis is an excellent question. For example, consider the word \"bank,\" which can refer to a financial institution or the side of a river. It will have the same token, but the model will learn, through the context provided by surrounding words, to associate it with a representation that aligns more closely with either the financial or the geographical meaning. However, it is the transformer that accomplishes this, not the tokenizer. \\n\\nYou mentioned during tokenization that we should retain the smaller tokens we started with. For instance, if you begin with the letter \"T,\" you keep the \"T\" and then build your tokenizer to the extent that you can input tokens. Suppose you did not train on the token \"token,\" but in your data, you are trying to encode it. How does the tokenizer know to encode it as \"token\" rather than just using \"T\"? \\n\\nThat is a great question. When you tokenize, which occurs after the tokenizer has been trained, you always choose the largest token that can be applied. Therefore, if \"token\" is available, you will never use \"T\"; you will always use \"token.\" \\n\\nHowever, there are many computational benefits and tricks that can enhance the efficiency of tokenization, which are not often discussed. I believe many people think we should move away from tokenizers altogether and tokenize character by character or byte by byte. Nonetheless, as I mentioned, there is currently an issue related to length. Perhaps in five to ten years, we will develop different architectures that do not scale quadratically with the length of the sequence, allowing us to move away from traditional tokenizers. \\n\\nCould you elaborate on the drawbacks of tokenizers? One pertinent example is mathematics. Currently, numbers are not tokenized effectively.',\n",
       " \"For example, the number 327 may have its own token, which implies that models interpret numbers differently than humans do. This discrepancy can be frustrating because our ability to generalize in mathematics relies on treating each character separately, allowing for composition where adding elements is equivalent to summing each individual component along with the respective unit. Consequently, specialized tokenization becomes necessary. \\n\\nOne significant advancement introduced by the GPT foundation is the modification of how code is tokenized. For instance, in Python, the common practice of using four spaces at the beginning of a line was previously handled in a somewhat unconventional manner, resulting in models struggling to comprehend code effectively. Thus, the role of tokenizers is critically important.\\n\\nNow, let us shift our focus to evaluation. The evaluation of large language models (LLMs) is typically conducted using a metric known as perplexity. At a high level, perplexity is essentially equivalent to validation loss. The distinction with perplexity lies in its use of a more interpretable measure: the average per-token loss, which is then exponentiated. The rationale for exponentiating is twofold: first, the loss function incorporates logarithmic values, and humans generally find it challenging to conceptualize in logarithmic terms; second, the base of the logarithm influences the interpretation of the results.\\n\\nBy exponentiating, we express everything in terms of vocabulary size. The average per-token loss ensures that perplexity remains independent of the sequence length. Thus, perplexity can be defined as two raised to the power of the average loss of the sequence. The range of perplexity spans from one to the length of the tokenizer's vocabulary. A perplexity of one indicates perfect prediction, where each word contributes a product of ones. Conversely, if predictions are entirely random, the perplexity equates to the size of the vocabulary. Therefore, the intuition behind perplexity is that it represents the number of tokens among which the model is uncertain.\",\n",
       " 'If your model is perfect, it does not hesitate; it knows exactly which word to predict. Conversely, if it lacks certainty, it hesitates among all possible vocabulary options. Perplexity has significantly improved; on a standard dataset from 2017 to 2023, it decreased from approximately 70 tokens to less than 10 tokens over these five to six years. This indicates that models previously averaged around 70 words when generating a word, whereas now they average fewer than 10 words, which represents a substantial improvement. \\n\\nHowever, perplexity is no longer commonly used in academic benchmarking, primarily because it is dependent on the tokenizers employed and the specific datasets on which evaluations are conducted. Nonetheless, it remains crucial for the development of large language models (LLMs). When training your own LLM, researchers still closely examine perplexity. \\n\\nA more prevalent method in academia for evaluating LLMs involves aggregating results from various classical natural language processing (NLP) benchmarks. I will provide a few examples shortly. The goal is to collect as many automatically evaluable benchmarks as possible and assess performance across all of them. Two notable benchmarks are HELM, developed by Stanford, and the Hugging Face Open LLM Leaderboard, which are currently among the most widely used.\\n\\nTo illustrate, HELM encompasses a variety of tasks, primarily those that can be easily evaluated, such as question answering. The advantage of question answering is that the correct answer is typically known. The evaluation process involves assessing how likely the language model is to generate the correct answer compared to alternative responses. This is essentially the high-level approach to evaluating these models.\\n\\nFor a specific example, the MMLU benchmark is arguably the most common academic benchmark for LLMs. It consists of a collection of numerous questions and answers across various domains, including college-level medicine, physics, and astronomy. An example question in astronomy might be, \"What is true for Type Ia supernovae?\" The model is then presented with four potential answers, and it is tasked with determining which one is the most likely correct response.',\n",
       " \"There are various methods for evaluating model responses. One can assess the likelihood of generating all possible answers or inquire which response the model deems most probable. While there are different prompting techniques, one can ascertain the correct answer at a high level. Additionally, there are three common errors to consider. \\n\\nWhen generating unconstrained text as output, a pertinent question arises: how does one evaluate a model that produces semantically identical responses that do not match the exact tokens? This is an important topic that I will address later.\\n\\nIn this context, we do not employ unconstrained outputs. The evaluation of the Massive Multitask Language Understanding (MMLU) benchmark involves either asking the first question and examining the likelihood of the model generating options A, B, C, and D, or directly querying the model to identify which of these options is most likely. In this way, we can determine whether the most probable next token corresponds to A, B, C, or D. Thus, we constrain the model's responses to these four choices through the prompt.\\n\\nWhen you mention constraining the model, are you referring to limiting its entire probability distribution to only compare these specific outputs? In the second scenario I described, you would indeed implement both strategies: prompting the model with A, B, C, or D while also restricting the evaluation to these four tokens.\\n\\nIn the first scenario, no generation is necessary. Given that it is a language model capable of producing a distribution over sentences, one simply examines the likelihood of generating each word. Specifically, one assesses the likelihood of the second choice and determines whether the most probable sentence aligns with the correct answer.\\n\\nThus, sampling is not required; instead, one utilizes the probability P of X1 to XL. Does that clarify the concept? That said, the evaluation of open-ended questions is a topic we will explore later, and it presents significant challenges.\\n\\nEarlier, you mentioned that metrics such as perplexity are not typically employed due to their dependence on tokenization and other design choices. I would be happy to elaborate on that. Perplexity, as I previously indicated, ranges from one to the size of the vocabulary.\",\n",
       " \"Now, imagine that ChatGPT utilizes a tokenizer with approximately 10,000 tokens, whereas Gemini from Google employs a tokenizer with 100,000 potential tokens. Consequently, the upper bound of perplexity achievable with Gemini may actually be inferior to that of ChatGPT. Does that make sense? This serves as a preliminary illustration of the significance of the tokenizer. \\n\\nNow, regarding evaluation challenges, there are numerous issues to consider, but I will briefly address two. As I mentioned, there are multiple methods for evaluating these MMLUs; I provided two examples. Historically, despite being a classical benchmark widely used, different companies and organizations have employed varying evaluation methods for MMLU, resulting in significantly divergent outcomes. For instance, the Llama 65B model, which was the first in Meta's Llama series, achieved an accuracy of 63.7 on the Helm benchmark, yet only 48.8 on another benchmark. Thus, the manner in which models are evaluated is crucial, and this does not even account for the complexities introduced by prompting, which is another significant issue. \\n\\nThere are many inconsistencies in evaluation; it is not as straightforward as it may appear. One pertinent question is how we can ensure that these models are not trained on the benchmark data. \\n\\nThe second issue, which is an excellent question, pertains to chain test contamination. This is particularly important in academia, especially given that this discussion primarily revolves around training large language models. For companies, this may be less critical since they are aware of their training data. However, for us, this presents a genuine challenge. \\n\\nThere are various methods to ascertain whether the test set was included in the training set. One clever approach discovered by researchers in Tetsu's lab involves recognizing that most online datasets are not randomized. Since language models predict the next word, one can examine the entire test set. For example, what if we generate all the examples in sequential order versus in a different order?\",\n",
       " 'If it is more likely to generate a response in a particular order, despite the absence of a definitive structure, it suggests that such content was likely included in the training set. Does that make sense? This is just one example; there are numerous other approaches to consider. Train-test contamination, while not critical for development, is particularly significant for academic benchmarking. There are many other challenges, but I will move on for now.\\n\\nData is another substantial topic. At a high level, people often state that large language models are trained on the entirety of the Internet. However, what does that actually mean? Some may refer to it as the \"clean Internet,\" which is even less clearly defined. The Internet is quite chaotic and not representative of what we desire in practice. If I were to download a random website right now, you would likely be astonished by its content; it is certainly not akin to Wikipedia. I will briefly outline what is typically done in this context, and I can address any questions afterward, but it is important to note that data, in and of itself, is a vast topic.\\n\\nEssentially, the first step involves downloading a comprehensive dataset of the Internet. This is accomplished using web crawlers that traverse every webpage available on the Internet or at least those indexed by Google, which currently amounts to approximately 250 billion pages and around 1 petabyte of data. A common approach is to utilize existing web crawlers rather than developing new ones. One widely used tool is Common Crawl, which updates its dataset monthly by incorporating newly discovered websites indexed by Google, resulting in a substantial repository of data.\\n\\nOnce you have this dataset, consider a randomly selected webpage from Common Crawl. It may not resemble the type of content one typically expects. For instance, this is an HTML page; while it may be difficult to discern, if you examine it closely, you will find some content. For example, it states, \"Test King World is your ultimate source for the System X high-performance server,\" followed by an ellipsis, indicating that the sentence is incomplete. This exemplifies the nature of random Internet content. Consequently, it is not particularly useful to train a large language model on such material. What are some of the necessary steps to address this issue?',\n",
       " 'First, one must extract the text from the HTML. This process presents numerous challenges. For instance, extracting mathematical content is particularly complicated yet crucial for training large language models. Additionally, boilerplate text is prevalent; many forums contain similar headers and footers, which should not be redundantly included in the dataset. Subsequently, undesirable content must be filtered out, including not safe for work material and personally identifiable information (PII). Typically, each organization maintains a blacklist of websites from which they prefer not to source training data. This blacklist is extensive, and any content originating from these sites is excluded from training.\\n\\nThere are alternative methods to address these issues, such as training a smaller model to classify PII and remove such content. Each aspect of this process is labor-intensive, and I will briefly outline them. The next step involves deduplication. As previously mentioned, headers and footers in forums often repeat, necessitating their removal. Furthermore, one may encounter numerous URLs that lead to the same website, as well as paragraphs from common texts that have been duplicated thousands of times across the Internet. Deduplication is particularly challenging due to the need to perform it at scale.\\n\\nOnce deduplication is complete, heuristic filtering is employed to eliminate low-quality documents. This can be achieved through rule-based filtering. For example, if the token distribution on a website significantly deviates from the expected distribution, it may indicate an outlier. Similarly, if the average word length on a site is excessively long, it may suggest anomalies. Conversely, if a website contains only three words, it may not warrant inclusion in the training set. Conversely, a site with an unusually high word count, such as ten million words, may also raise concerns about its quality. Numerous rules can be applied in this context.\\n\\nOne might question why we filter out undesirable content from our dataset instead of incorporating it as a supervised loss. Could we not simply identify a website that promotes hate speech and actively penalize it? While we will indeed implement such measures, they will occur in the post-training phase rather than at this initial step.',\n",
       " \"Pre-training involves the intention to model human speech patterns while eliminating extraneous elements such as headers, footers, and menus. This concept is indeed valuable and will be implemented in subsequent steps. The next phase is model-based filtering. \\n\\nOnce a substantial amount of data has been filtered, a useful technique involves utilizing the entirety of Wikipedia and examining all the links present within its pages. The rationale is that if a document is referenced by Wikipedia, it is likely to originate from a high-quality website. Consequently, one would train a classifier to predict whether a document is derived from these Wikipedia references or from the broader web. The objective is to prioritize documents that are linked to Wikipedia references.\\n\\nDoes that make sense? You will train a machine learning model, typically employing relatively simple models to facilitate scalability, especially considering the vast number of pages—approximately 250 billion. The next step involves classifying the data into various domains. For instance, one might categorize data as pertaining to entertainment, literature, or programming. \\n\\nSubsequently, you will adjust the weighting of these domains. For example, it may be observed that increased training on programming data enhances the model's reasoning capabilities. This notion is often expressed in a somewhat vague manner; however, it is generally accepted that training the model more extensively on programming content improves its reasoning skills. Thus, one would increase the weight of the programming distribution to bolster general language modeling abilities.\\n\\nLiterature is another domain that is frequently given additional weight, while entertainment content is often down-weighted. Such adjustments are essential. Historically, these modifications were made heuristically, but now there are comprehensive pipelines that automate these processes to some extent. \\n\\nAt the conclusion of training, it is customary to focus on high-quality data. This phase typically involves reducing the learning rate, which effectively leads to overfitting the model on high-quality data. In practice, this means overfitting on Wikipedia and other human-generated data that has been collected. Additionally, continual pre-training is employed to accommodate longer context lengths.\",\n",
       " 'I will skip over several points, but I want to convey the significant challenges associated with the assertion that one can simply train on Internet data; this is a considerable undertaking, and we have yet to fully resolve these issues. Collecting diverse data is a crucial aspect of practical large language model development, and some might argue that it is indeed the key factor.\\n\\nRegarding data, a fundamental question arises: when starting with petabytes of data, what is the typical amount of data retained after processing through all the necessary steps? Additionally, how large is the team typically required to navigate these data processing steps? Specifically, how extensive is the dataset after filtration, and what is the size of the team needed to manage the filtration processes you mentioned? \\n\\nThat is an excellent question. I will address the data size and the number of personnel involved. I am not entirely certain of the exact figures, but I would estimate that the dataset is likely larger than the number of individuals involved in the tuning and pre-training of the model. In the case of the Llama team, which comprises approximately 70 members, I would estimate that around 15 focus on data-related tasks. \\n\\nWhile not all aspects require a large number of personnel, substantial computational resources are necessary, particularly in terms of CPUs for data processing. I will address the second question at the end of this slide. As I previously mentioned, we have not yet solved the data challenges associated with pre-training. There remains a significant amount of research to be conducted. First, we need to determine how to process data efficiently. Second, we must consider how to balance the various domains effectively. The potential for synthetic data generation is a particularly pertinent topic at present, which we will discuss further later.',\n",
       " 'The availability of data on the Internet is often insufficient. Can multimodal data be utilized in place of solely text data, and how does this enhance text performance? There exists considerable secrecy surrounding these matters, as they are central to the functioning of most pre-trained large language models. In terms of competitive dynamics, companies typically refrain from disclosing their data collection methodologies, partly due to concerns regarding copyright liability. They are unlikely to reveal that they have trained on books, despite having done so, as this could expose them to legal action.\\n\\nRegarding common academic benchmarks, this addresses your inquiry. Initially, these benchmarks began with approximately 150 billion tokens, equating to around 800 gigabytes of data. Currently, the scale has expanded to approximately 15 trillion tokens, which corresponds to the size of the most advanced models available today. This represents a magnitude increase of roughly two orders, translating to about 80 exabytes of data. Consequently, this indicates a filtering process of 100 to 1,000 times the original Common Crawl dataset, if I am not mistaken.\\n\\nA notable benchmark is \"The Pile,\" which serves as an academic standard. The data distribution includes sources such as Archive, PubMed Central—covering biological content—Wikipedia, Stack Exchange, GitHub, and various books. However, this dataset remains relatively small. For context, while it is represented as 280 petabytes, the actual size is approximately 100 times larger. It is important to note that the proportion of GitHub and Wikipedia content is limited in closed-source models. For instance, LLaMA 2 was trained on 2 trillion tokens, while LLaMA 3 was trained on 15 trillion tokens, making it the most substantial model we are aware of in terms of training data. The largest academic benchmark also comprises 15 trillion tokens.\\n\\nAs for GPT-4, its training data remains largely unknown, but it is likely within a similar range, estimated around 13 trillion tokens based on leaks, assuming those leaks are accurate. Are there any further questions regarding data before we transition to the topic of scaling laws?',\n",
       " 'I apologize for the abundance of information, but training a large language model involves numerous complexities. The concept of scaling laws is particularly significant. Observations made around 2020, and supported by both theoretical and empirical evidence, indicate that increasing the amount of data used for training and enlarging the model size correlates with improved performance. This finding contrasts with the principles discussed in this class regarding overfitting. In the context of large language models, overfitting is less of a concern; larger models indeed yield better performance. It took considerable time for the academic community to recognize this distinction. However, for the purposes of the examination, it is essential to acknowledge that overfitting does exist.\\n\\nThe essence of scaling laws is that, given the understanding that more data and larger models consistently enhance performance, we can predict the extent of this improvement. Remarkably, this predictive capability holds true. In a notable paper titled \"Scaling Laws\" from OpenAI, three plots illustrate this phenomenon. The x-axis represents compute, indicating the computational resources expended during training, while the y-axis reflects test loss, which, although not perplexity, serves as a measure of validation loss. When both axes are plotted on a logarithmic scale, the relationship reveals a linear scaling law. This implies that an increase in compute by a specific factor will correspond to a predictable decrease in test loss. The same principle applies to dataset size and the number of parameters; increasing the dataset size will lead to a predictable reduction in loss, as will increasing the number of parameters.\\n\\nThis finding is remarkable and somewhat surprising. While the plots may appear straightforward, they suggest a profound implication: we can forecast our performance in the coming years based on the computational resources we anticipate adding, assuming these relationships remain valid. There is no theoretical ambiguity in this assertion. \\n\\nI would like to clarify two points. First, what specific loss metric is being utilized in this context? Is it perplexity? I previously mentioned that perplexity can be expressed as two raised to the power of the loss.',\n",
       " 'The power of perplexity is significant. Furthermore, when one increases the number of parameters or the total dataset size, does this not inherently increase computational requirements? This is an excellent question. The computational demand is indeed a function of two factors: the data and the parameters. \\n\\nWhat I am illustrating here is that if you increase the number of parameters, you should also increase the amount of data available. It is important to note that one does not repeatedly access the same dataset; this practice is not common, particularly in large-scale applications, as we currently possess sufficient data. Thus, the overarching trend remains: increased computation correlates with decreased loss. \\n\\nHave we observed the metrics from the past two years? The trend continues to hold. While I do not have specific figures to present, the evidence remains surprisingly consistent. Is there any empirical evidence suggesting a potential plateau? Theoretically, we would not anticipate a plateau. Currently, there is no empirical evidence indicating that we will reach a plateau in the near future. Why is this the case? We do not have a definitive answer. Will it eventually occur? Likely, but it is not a necessity, as the relationship is logarithmic. Mathematically, it could continue to decrease indefinitely. Most experts believe that a plateau will likely be reached at some point, though we cannot predict when.\\n\\nNow, let us delve deeper into scaling laws. Why are scaling laws particularly intriguing? Imagine that I provide you with 10,000 GPUs for a month. What model would you choose to train? How would you approach answering that question? This scenario is hypothetical, yet it reflects the challenges faced by companies today. The traditional pipeline involved tuning hyperparameters on large models. For instance, if I have 30 days, I would train 30 models for one day each, selecting the best one to use in production. This implies that the model ultimately deployed was trained for only one day. The new pipeline, however, begins with identifying a scaling recipe.',\n",
       " 'One common observation is that when increasing the size of a model, it is advisable to decrease the learning rate. Thus, one can establish a scaling recipe indicating that if the model size is increased, specific hyperparameters should be adjusted accordingly. Subsequently, hyperparameter tuning is conducted on smaller models of varying sizes. For instance, I might train multiple models over a period of 30 days, each with different configurations, and perform hyperparameter tuning on these smaller models.\\n\\nAfterward, I will fit a scaling law to these models and attempt to extrapolate which configuration would yield the best performance if applied to a larger model. Consequently, I would train the final, significantly larger model for 27 days, rather than just one day. The revised approach emphasizes conducting hyperparameter tuning on smaller models at different scales, rather than on the full-scale model intended for practical use. This allows for predictions regarding their performance once scaled up.\\n\\nTo illustrate this process, consider the comparison between transformers and LSTMs. Suppose you have access to 10,000 GPUs and are uncertain whether to utilize a transformer-based model or an LSTM-based model. In this case, I would train transformers at various scales, plotting the number of parameters on the x-axis and the test loss on the y-axis. I would then train LSTMs at different scales as well. Once I have gathered these data points, I would observe that they tend to conform to a scaling law. By fitting this scaling law, I would be able to predict the performance of the LSTM if I had ten times more computational resources.\\n\\nIt is worth noting that the relationship for LSTMs is somewhat less linear, but one could still make predictions about their performance. From the resulting plot, it would be evident that transformers outperform LSTMs. When analyzing such scaling laws, two critical factors must be considered: the scaling rate, which corresponds to the slope of the scaling law, and the intercept. It is possible to begin with a model that performs poorly but improves over time. In the case of LSTMs, they tend to perform worse in both aspects. However, there are instances where predictions indicate that beyond a certain scale, one model type may be more advantageous than others. This underscores the utility of scaling laws. Are there any questions regarding this topic?',\n",
       " 'The sensitivity of performance to minor architectural differences, such as variations between different transformer architectures, is a critical consideration. One must often fit a scaling curve and determine that the scaling model suggests a logarithmic function, which can then be extrapolated for specific applications. For instance, if one is an academic proposing a new activation function, the typical approach would involve fitting a scaling law and comparing it against a standard, such as the Gaussian Error Linear Unit (GELU), to demonstrate superiority. However, upon deeper reflection within the context of scaling laws, it becomes evident that the minor architectural differences primarily affect the intercept of the performance curve, which is relatively inconsequential. In practice, extending the training duration or utilizing more advanced computational resources can mitigate these differences, underscoring the notion that excessive focus on architectural nuances and loss functions may be misplaced. \\n\\nIn contrast, the quality of the training data is paramount; utilizing high-quality data significantly enhances scaling performance compared to inferior data. Another intriguing application of scaling laws is the optimal allocation of training resources. One might question whether to train larger models, given their demonstrated advantages, or to utilize more data, which also yields improved outcomes. This dilemma raises the question of whether to train a smaller model on a larger dataset or a larger model on a smaller dataset. The influential paper \"Chinchilla\" first addressed this issue, providing valuable insights into the relationship between model size and data quantity. \\n\\nTo illustrate this, one can examine plots depicting training loss, where the x-axis represents the number of parameters, indicating model size. The curves on these plots are referred to as ISO flops, signifying that all models along a given curve have been trained with an equivalent amount of computational resources. This is achieved by varying the number of tokens used during training in conjunction with the model sizes.',\n",
       " 'However, one must vary in such a way that the total computational resources remain constant. The curves you observe, represented in different colors, correspond to varying amounts of computation on which they were trained. Once the optimal point for each of these curves is identified, one can plot the number of floating-point operations per second (FLOPS) alongside the corresponding curve and the number of parameters utilized for training at that specific point. This data can then be represented on a log-log scale.\\n\\nSubsequently, one can fit a scaling law to this information. For instance, if one aims to train a model requiring 10 to the power of 23 FLOPS, the scaling law will indicate the precise number of parameters to employ, which may be around 100 billion. This analysis can similarly be applied to FLOPS and tokens, allowing one to predict the appropriate model size based on a specified computational budget, such as one month of compute.\\n\\nWhile this theoretical framework appears elegant, it is important to acknowledge the complexities involved, such as whether to include embedding parameters in the calculations. Nevertheless, if executed correctly, these principles do hold true. The optimal number of tokens identified by the researchers associated with the \"Chinchilla\" study is 20 tokens for every parameter trained. Therefore, for each additional parameter, one should train the model on an additional 20 tokens.\\n\\nIt is crucial to note that this represents optimal training resources. For example, if one has 10 to the power of 23 FLOPS or a budget of approximately 5 million dollars to train the model that achieves the lowest loss, one must consider the implications of inference costs as well. Companies must account for the fact that smaller models incur lower expenses over time. Consequently, when factoring in inference costs, other studies suggest that the optimal ratio is around 150 tokens per parameter, as smaller models tend to be more cost-effective in the long run. This ratio reflects the current best practices for models utilized in production.\\n\\nAre there any questions regarding the \"Chinchilla\" findings? \\n\\nIn practice, how expensive is inference for these models relative to training? In fact, it is quite costly. However, I will refrain from discussing inference in detail, as it would require an entirely separate lecture.',\n",
       " 'Consider ChatGPT, which currently has approximately 600 million users. This represents a substantial user base. The operational costs associated with such models are considerable; however, there are numerous optimization strategies available for inference. This topic warrants an entire lecture of its own, so I will refrain from delving into it at this moment.\\n\\nNow, regarding tuning, as I mentioned earlier, scaling laws can provide insights into various aspects of model training. I have presented two examples, but the scope is much broader. Questions arise concerning the data utilized: what mixture and weighting should be applied? These considerations relate to our previous discussions. Additionally, one must contemplate the architecture employed—whether to expand the model\\'s width or depth, whether to invest in additional GPUs, or to focus on data collection. All of these factors can be addressed through scaling laws.\\n\\nI would like to highlight a concept known as the \"bitter lesson,\" articulated by Richard Sutton in a notable blog post from 2019. He observed, a realization that I believe is not sufficiently acknowledged, that the presence of scaling laws indicates that increased computational resources will yield superior models. Thus, with greater scale, one can expect improved model performance. Furthermore, according to Moore\\'s Law and its variants, advancements in computational power are inevitable. Consequently, the critical factor becomes the development of architectures capable of leveraging this computational capacity. Therefore, the emphasis should be on systems and data rather than on minor architectural differences, such as activation functions. This perspective explains why much of the research tends to focus on aspects that may be less relevant to industry applications. I, too, was among those researchers for a significant portion of my career. \\n\\nIt is advisable to avoid overcomplicating matters. Focus on executing the fundamental tasks effectively. This is a lesson that OpenAI has imparted through the development of ChatGPT and its predecessors.\\n\\nNow, I would like to provide a rough estimation of the computational costs associated with training certain models. While I may be imprecise in some calculations, my intention is to convey a general understanding of the expenses involved. For instance, consider LLaMA 3, a model with 340 billion parameters, which is currently regarded as the best open-source model available. It was trained on 15.6 trillion tokens. To contextualize this, the optimal ratio of tokens per parameter is approximately 40, which is slightly higher than that of the Chinchilla model but lower than that of the inference-optimized model. For training optimality, the floating-point operations per second (FLOPS) for this model can be calculated as six times the number of parameters multiplied by the number of data points used in training.',\n",
       " 'If one performs a straightforward calculation, the result is approximately 3.8 x 10^25 floating-point operations per second (FLOPs). This figure is significant because, as noted in recent news, there exists an executive order from President Biden stipulating that models with one x 10^26 parameters, or FLOPs, are subject to special scrutiny. Consequently, the threshold has been set at half of that value, ensuring that models remain just below this limit to avoid additional oversight.\\n\\nThe parameter P represents the number of parameters, while N denotes the number of tokens in the dataset. This is merely an approximation. We know that the model was trained on 16,000 H100 GPUs, and we are aware of the throughput they configured.\\n\\nUpon conducting the computation, it is estimated that the training process takes around 70 days or approximately 26 million GPU hours, based on my preliminary calculations. The actual reported usage was 30 million GPU hours, suggesting that they may have encountered some challenges, though the precise reasons remain unclear. Nevertheless, following the straightforward computation, the training duration is around 70 days.\\n\\nEstimating the rental cost for H100 GPUs, if one were to rent that many GPUs for that duration, the lower bound on the rental cost is approximately $2 per hour. Multiplying this by 26 million hours yields a total of $52 million. While they likely paid somewhat less than this amount, it is unlikely to be significantly lower, as GPU rental services typically operate with narrow profit margins.\\n\\nRegarding salaries, I estimated 50 employees at an annual salary of $500,000 each, which places the total around $25 million. Therefore, when aggregating these figures, the total cost for training this language model is approximately $75 million, though I may be off by about $10 million. \\n\\nIn terms of carbon emissions, many may inquire about the environmental impact, which is not solely a financial concern. My calculations indicate that the training process emits around 4,400 tons of CO2 equivalent. To put this into perspective, this amount corresponds to approximately 2,000 round-trip tickets from JFK to London. Currently, while the carbon emissions are substantial, they do not yet represent a critical issue. However, as we progress to models like GPT-6 or GPT-7, where emissions could potentially increase by a factor of one hundred, this may become a significant concern. At present, it remains relatively manageable within the broader context.\\n\\nAs for future models, it is essential to consider that with each new generation, the number of FLOPs is expected to multiply by a factor of ten, assuming sufficient energy availability and the procurement of adequate GPU resources.',\n",
       " 'Are there any questions regarding the back-of-the-envelope calculations? No? Very well. We have discussed pre-training, and I would now like to address systems, as we have established the critical importance of computational resources. This raises the question of how to optimize computation. I will defer this topic until the end, as I am uncertain about the time we will have available. While it is an important subject, it diverges slightly from our current discussion. Therefore, I will proceed to post-training for now.\\n\\nThe purpose of post-training is to develop AI assistants. Language modeling alone is insufficient for the functionality of an AI assistant. For instance, if one queries GPT-3, which is a purely language model and not an aligned one, with a request such as, \"Explain the moon landing to a six-year-old,\" the response may resemble an explanation of the theory of gravity instead. This occurs because the model has learned from the internet that when posed with one question, it often encounters a series of related questions rather than a direct question-and-answer format. This is not the desired outcome for an AI assistant.\\n\\nSo, how do we achieve this alignment, which is the essence of post-training and transforming these models into assistants? The objective of this alignment is to ensure that large language models (LLMs) adhere to the instructions provided by users and possibly reflect the intentions of their designers. Consider moderation; for example, OpenAI does not want its model to generate toxic content. On the left side, you can observe that when a question is posed, the model provides a relevant answer, unlike the previous iterations of LLMs. Conversely, on the right side, if you request the model to compose a tweet characterizing a certain segment of the population as malevolent, it will respond that it cannot fulfill that request. This exemplifies the alignment process.\\n\\nThe underlying principle here is that the data required for training these models is well-defined; we know what we want, which is to solicit input from humans.',\n",
       " 'This is a question, and this is the answer that you seek. However, the challenge lies in the fact that collecting such data is quite expensive and difficult to locate online. In contrast, pre-training data may not be precisely what you desire, but it is abundant. Therefore, the primary approach is to utilize a pre-trained large language model, which has been trained on a vast corpus of internet data, and then fine-tune it. This process involves making slight adjustments to the model\\'s weights based on the specific type of data you require. Given that the model has already been pre-trained on extensive internet data, it possesses a foundational understanding of English and standard language syntax, allowing for effective fine-tuning with minimal additional data.\\n\\nSupervised fine-tuning, or SFT, is precisely what I have described; it involves fine-tuning the large language model using the desired responses collected from human sources. The term \"supervised fine-tuning\" is employed because the objective is to perform language modeling based on authentic answers. Language modeling, in this context, refers to the task of predicting the next word in a sequence, which constitutes the fine-tuning aspect. This process is conducted using the desired answers provided by humans, hence the designation of \"supervised.\"\\n\\nSo, how do we collect this data? As I mentioned, we simply ask humans to provide responses. For instance, one might pose the question, \"Can you write a short introduction about the relevance of the term \\'monopsony\\'?\" The expected response might be, \"Monopsony refers to a market structure...\" This example illustrates a human-generated response. In fact, this process is facilitated by platforms like Open Assistant, which serve as a means to gather data from human contributors.\\n\\nThis method of supervised fine-tuning or alignment is fundamentally what distinguishes ChatGPT. It represents a significant advancement from GPT-3, which was primarily recognized within the AI research community, to ChatGPT, which gained widespread recognition among the general public. \\n\\nHowever, the challenge with human-generated data is that it is slow to collect and costly. One potential solution is to leverage large language models to scale the data collection process. This was precisely the approach we adopted with Alpaca one year ago. We utilized a dataset comprising 175 question-answer pairs, which were generated by humans.',\n",
       " 'We utilized the best model available at the time, specifically Text-Davinci-003, to generate a substantial number of question-and-answer pairs. Essentially, we instructed the model to produce responses that mirrored human-written content. In total, we collected 52,000 question-and-answer pairs generated by the large language model (LLM). Subsequently, we employed the LLaMA 7B model, which was the most effective pre-trained model at that time, and we fine-tuned it using supervised fine-tuning, as previously mentioned. This process culminated in the development of the Alpaca 7B model. \\n\\nThe data we gathered included examples such as, \"What does algorithm mean?\" with the response being, \"An algorithm is a step-by-step set of instructions used to solve a problem or achieve a goal.\" The quality of the data is quite satisfactory, considering it was generated by LLMs from earlier iterations. This endeavor marked, at least for us, an academic replication of ChatGPT.\\n\\nCurrently, there exists a significant field focused on synthetic data generation, which explores how LLMs can expedite the development of other LLMs by reducing the amount of human labor required for data collection. We have discussed the types of data we collect and the methods employed. One surprising finding regarding supervised fine-tuning (SFT) is that a large volume of data is not necessarily required. The research presented in the paper titled \"LIMA\" indicates that increasing the amount of data used for SFT from 2,000 to 32,000 examples yields minimal benefits. In this context, scaling laws do not provide substantial advantages.\\n\\nThe underlying intuition is that what is learned through this process primarily involves formatting desired answers. In other words, pre-trained models essentially capture the distribution of responses from various users on the internet. Some users may prefer bullet points, while others may favor a question-and-answer format. Thus, the model is guided to optimize for a specific type of user rather than being taught new information through SFT.\\n\\nIn essence, supervised fine-tuning instructs the model to specialize in optimizing for one type of user that it has already encountered in the pre-trained dataset. The knowledge is inherently present in the pre-trained LLM, and the fine-tuning process merely refines its focus toward a particular user type. Are there any questions regarding supervised fine-tuning?',\n",
       " 'Indeed, it is a significant concern with synthetic data that if one continues to generate data from the same distribution, eventually, one is not learning a new distribution but merely manipulating the existing one and bootstrapping from it. It is evident that this approach cannot be sustained indefinitely. One cannot persist in generating data from the same distribution and expect to acquire novel insights.\\n\\nAre there ongoing research activities addressing this issue? I would appreciate any thoughts you may have regarding how researchers are conceptualizing this challenge and exploring better methods for bootstrapping, or perhaps reconsidering the necessity of extensive data generation, as the data suggests that a limited number of high-quality human-generated examples—perhaps around 2,000—may suffice.\\n\\nThis raises a pertinent question. Regarding the data aspect, I would argue that while it is not critically important for supervised fine-tuning (SFT), there is another dimension we will discuss shortly where data does indeed matter. My intuition, albeit based on limited empirical evidence, is that even when utilizing large language models (LLMs), if one relies solely on text generated by LLMs over three or four generations, the potential for significant improvement diminishes. However, what I find crucial is the integration of human input alongside LLMs.\\n\\nThe ideal approach may not involve exclusively using LLMs or solely human-generated content, but rather a collaborative model where the LLM generates new text and humans provide edits. Edits are considerably faster to produce than writing entire texts from scratch. I believe that this type of collaboration can yield additional information from an information-theoretical perspective while maintaining efficiency compared to relying solely on human input. As a field, we are likely to gravitate toward such methodologies, which essentially involve identifying critical examples and soliciting human input precisely when needed.\\n\\nRegarding the training process, do we employ the same loss function and general training algorithm for the supervised fine-tuning phase as we do for pre-training? \\n\\nThe examples you presented highlight the importance of factual accuracy. In more complex scenarios, we still utilize the same training loss. I may not have emphasized this sufficiently earlier. The process is fundamentally about language modeling; we fine-tune the LLM using the language model on the desired responses. This initial step of SFT employs the same loss function, where the objective is to specialize in that specific type of data.',\n",
       " 'There is a pertinent question regarding the definitions of pre-training and post-training. In essence, these terms refer to the different types of data utilized. The designation of \"post-training\" arises from the distinct manner in which we collect this data. These are indeed excellent questions. \\n\\nWhile they may appear similar, one might wonder why these 2,000 examples exert such a disproportionately large influence. This is another reason we refer to it as post-training; we employ a different set of hyperparameters. As I mentioned earlier, at the conclusion of pre-training, the learning rate effectively approaches zero. In contrast, during post-training, we increase the learning rate to approximately 1E-5. Consequently, the weight assigned to these examples differs significantly.\\n\\nThe second aspect of post-training involves what we term reinforcement learning from human feedback, or RLHF. Some of you may be familiar with this concept. The challenge with supervised fine-tuning (SFT) is that it relies on behavioral cloning, which entails mimicking human responses. This approach presents several issues. \\n\\nOne significant limitation is that the model is constrained by human capabilities. For instance, while humans can appreciate the quality of a book and may express preferences, they are unlikely to produce the optimal content they would personally wish to read. Thus, the model\\'s output is restricted by the human ability to generate responses, even though humans may excel at discerning quality.\\n\\nAnother intriguing issue relates to the phenomenon of \"hallucination,\" wherein large language models (LLMs) generate false or misleading information. Some researchers have hypothesized that this issue may stem from supervised fine-tuning. Even when SFT is conducted on accurate data, the model may not learn anything new. If a human provides an answer that the model was previously unaware of, the model interprets this input as a directive to generate a response that seems plausible, despite lacking certainty regarding its truthfulness.',\n",
       " \"To provide a concrete example, let us revisit the monopsony scenario. Consider the task of generating content regarding monopsony. Imagine that a human authored a reference related to this topic. While it is possible that such a book exists and the reference is accurate, if the large language model (LLM) did not encounter this reference during pre-training, it would lack the knowledge to recognize it as correct. Consequently, what the model generates may be a plausibly sounding reference rather than the actual reference it encountered during pre-training. This phenomenon, known as hallucination, may indeed be a consequence of supervised fine-tuning. \\n\\nMoving on to the third issue, we must address the cost associated with generating ideal responses. This ties back to the earlier question regarding the expense of having humans compose entire answers. This is where reinforcement learning from human feedback (RLHF) becomes relevant. The premise is that instead of merely replicating human behaviors, we aim to maximize human preferences. \\n\\nThe process involves generating two responses for each instruction posed to the model. Typically, a well-tuned model is employed rather than a raw LLM, as the fine-tuned LLM is more likely to produce satisfactory answers. Subsequently, labelers are tasked with determining which of the two responses is superior. Utilizing various algorithms, which we will discuss shortly, the model is fine-tuned to produce more of the preferred response than the less favored one.\\n\\nNow, the question arises: how do we achieve this? We will explore two primary methods commonly utilized within the community. The first method involves the application of reinforcement learning. It is essential to consider what reward we are optimizing in this context. There are two potential options. One approach is to compare the output generated by a baseline model with that produced by our model. We can then solicit human feedback to ascertain which output is preferable, using this feedback as a reward mechanism. If our model's output surpasses the baseline, it receives a positive reward; if not, it incurs a negative reward. This creates a binary reward system. However, the challenge with binary rewards is their sparsity, which limits the information we can glean from them. For instance, while one response may be slightly better than another, the binary system does not capture the nuances of this difference effectively.\",\n",
       " 'You cannot ascertain the extent of improvement from this alone. An alternative approach is to train what we refer to as a reward model, which functions as a classifier. In this context, machine learning is employed to classify the relative quality of two outputs from the perspective of human preference. This process is somewhat meta; essentially, you train a reward model, denoted as R, which is a large classifier. You provide this reward model with the input and one of the two outputs, and you exponentiate that output. This corresponds to the softmax function that you are all familiar with. Subsequently, you divide by the exponentiated reward of the first output, and this is applied to the second output during training. \\n\\nThe rationale behind this approach is to enable your reward model to classify how much better one output is compared to another. A more straightforward way to articulate this is that your reward model will yield a reward that will serve as the logits for your softmax function. Therefore, if you obtain high logits from your softmax, it indicates a strong likelihood that this output is superior. This is referred to as the Bradley-Terry model.\\n\\nTo clarify, does this reward model evaluate the entire output, or does it focus on specific aspects? Yes, it evaluates the entire output at once, considering all inputs and outputs to produce a single numerical value.\\n\\nWith this reward model, where does the human element fit in? I apologize for any lack of clarity. You train this reward model to align with the green and red preferences expressed by humans. Essentially, you are training a classifier to determine whether humans prefer red or green. However, instead of relying on a binary reward, which reflects human feedback, you utilize the logits from the softmax function. The advantage of logits is that they are continuous values.\\n\\nThus, if your reward model indicates high logits, it suggests that, in some respects, the human preference strongly favored this answer over another. As I mentioned earlier, continuous information is indicative of quality, which is why this method is commonly employed in practice, or at least was in the past. I will discuss an alternative algorithm later.',\n",
       " 'At the conclusion of the process, one essentially employs reinforcement learning as understood in the context of the model. With the reward established, one samples the output generated by the large language model. Regularization terms are then applied to mitigate the risk of over-optimization. It is crucial to recognize that the reward model may not accurately represent human preferences; thus, one should avoid maximizing this reward to an extreme extent. This is accomplished using Proximal Policy Optimization (PPO), a widely utilized reinforcement learning algorithm. \\n\\nIt is important to note that, in this context, large language models function as a policy for reinforcement learning rather than maximizing maximum likelihood. Consequently, the models do not model any distribution in the traditional sense. This distinction is significant because models trained using PPO do not yield meaningful likelihoods of text. Instead, they are optimized solely for generating the most probable output, rather than for modeling the full range of responses that humans might provide. In other words, there is no incentive for the model to produce a diverse set of outputs; nothing encourages the model to generate responses with a degree of entropy.\\n\\nFor those who may not have followed this point closely, it is not critical, but it is beneficial to be aware of it. PPO is precisely the method employed by ChatGPT in its initial development. According to their blog post, the process consists of three steps: first, conduct supervised fine-tuning, which you are now familiar with; second, train a reward model based on human preferences; and third, implement multiple steps of PPO, represented by the blue arrow in their diagram. This iterative training process involves training the model with PPO, collecting new data, and continuing the cycle. This methodology represents a significant advancement from GPT-3 to ChatGPT.\\n\\nHowever, it is essential to acknowledge that PPO presents numerous challenges. While reinforcement learning is theoretically appealing, those who have worked with it in practice understand its complexities. Issues such as rollouts, outer loops, and clipping introduce significant complications, making the implementation quite intricate. The idealized version of PPO used in language model settings is already more complex than the expectations previously discussed, and in practice, it is even more complicated. We have developed one implementation of this algorithm, but I will not delve into the details. Essentially, there are numerous considerations to account for when implementing this type of PPO algorithm.',\n",
       " 'Clipping is pervasive, and the complexities involved are not well documented. To address this, a new method was proposed from Stanford one year ago, known as DPO, which essentially simplifies PPO. The central idea is that instead of employing reinforcement learning, one can maximize the probability of generating preferred outputs while minimizing the probability of generating undesired ones. In terms of human preference, this can be visualized as maximizing the \"green\" outcomes while minimizing the \"red\" ones.\\n\\nThe loss function is represented as the logarithm of the model\\'s likelihood, which indicates the probability of the model generating outputs that align with human preferences, given the inputs. The objective is to maximize the likelihood of producing preferred outcomes and minimize the likelihood of producing those that are not favored. The additional terms in the equation are not particularly significant; the core concept is straightforward: maximize what is liked and minimize the rest.\\n\\nIt is noteworthy that the remaining components are structured such that the global minima of PPO and DPO, under certain assumptions, are essentially equivalent. This approach is mathematically sound, although I will not delve into the derivations. It differs significantly from PPO, where the process involved collecting human preferences, training a reward model using maximum likelihood, and then applying reinforcement learning. In contrast, DPO simplifies this to a focus on maximum likelihood alone.\\n\\nThis method appears to be both simpler and more intuitive. A pertinent question arises: why was the reward model initially employed? What prompted this approach? This is indeed an excellent question. While I cannot provide a definitive answer, I can share that at OpenAI, the individuals who developed ChatGPT were also responsible for creating PPO. It seems that their background in reinforcement learning made this approach intuitive for them. There are also potential additional benefits to consider.',\n",
       " 'For example, consider the use of the reward model in reinforcement learning. A notable advantage of this approach is that it allows for the utilization of unlabeled data alongside the reward model. In contrast, when employing DPO, one is limited to using only labeled data. In the case of PPO, the process begins with training the reward model, after which unlabeled data can be employed, with the reward model effectively labeling this data. This presents additional potential for improvements in practice.\\n\\nIt is noteworthy that many individuals on this team are experts in reinforcement learning, including John Schulman, the principal author of PPO. The implementation of PPO is significantly simpler and performs comparably well. Consequently, this methodology has become the standard within the open-source community and is also widely adopted in industry, referred to as DPO gains. The older papers on the left illustrate this.\\n\\nIn a summarization task, the pre-trained models demonstrate satisfactory performance, which improves with scale. Supervised fine-tuning enhances performance further, while PPO or other methods incorporating reinforcement learning from human feedback (RLHF) yield results that, depending on the benchmark, can surpass human performance. The human reference summaries corroborate this observation. In our paper, Alpaca Farm, the evaluation is not of paramount importance; however, it is evident that the transition from pre-trained models to supervised fine-tuning (SFT) and subsequently to DPO shows that both DPO and PPO achieve identical performance levels. Thus, RLHF proves beneficial, leading to the conclusion that DPO is a straightforward data collection method.\\n\\nThe initial idea for collecting such data is to engage human participants. As previously discussed, the guidelines for what humans should label are quite complex, making the task challenging. If one undertakes any labeling, it becomes apparent how intricate the process is. For instance, consider the question, \"Tell me about self-driving cars.\" Two responses might be: \"Self-driving cars are vehicles that are capable of detecting their surroundings,\" and \"Self-driving cars are cars equipped with sensors to navigate without the need for a driver.\" Both responses appear acceptable, yet determining which is superior is not straightforward. Consequently, the challenge with human labeling arises from the tendency to optimize for various high-level features. For example, the second response is longer, and it is likely that most individuals would prefer it, even though the first response may be more effective. I have not analyzed it closely enough to make a definitive judgment.',\n",
       " 'The challenges associated with human involvement are twofold: they are slow and expensive. Furthermore, as previously mentioned, it is difficult to concentrate on aspects that truly matter, such as correctness. Instead, individuals often focus on less significant factors, such as form or length. Consequently, I demonstrate that with Reinforcement Learning from Human Feedback (RLHF), the more human feedback is incorporated, the longer the outputs of the models tend to become. If you have ever been frustrated by ChatGPT providing excessively lengthy responses, this phenomenon can be attributed to RLHF.\\n\\nAdditionally, the distribution of annotators is crucial; one must consider the characteristics of the humans we wish to represent in these models. Another pertinent issue is the ethics of crowdsourcing. Typically, the individuals performing the labeling are not compensated adequately and are often exposed to toxic data, as the objective is for the model to avoid generating such content. Thus, there are numerous ethical challenges associated with human data.\\n\\nLast year, we pursued a similar approach to that of Alpaca, addressing the challenges posed by human involvement by considering the possibility of replacing humans with large language models (LLMs). We replaced human preferences with LLM preferences. In the figure presented, the X-axis represents the cost incurred for collecting human data, approximately $300 for 1,000 examples, sourced from Mechanical Turk, which is generally less expensive than other available options. The Y-axis reflects the agreement among humans, specifically the mode of human responses.\\n\\nAs previously stated, labeling is inherently complex. Humans tend to agree with themselves only about 66% of the time on binary tests. This is not indicative of a lack of competence; indeed, the five main authors of this paper attempted to label the data ourselves and achieved only around 67 or 68% accuracy, despite engaging in extensive discussions about the labeling process. The task is undeniably intricate.\\n\\nIn this analysis, I present various models, demonstrating that they are significantly more cost-effective and can achieve higher agreement with the mode of human responses than humans themselves. This discrepancy arises because humans exhibit considerable variance, whereas models have minimal variance. While models may be somewhat biased, their reduced variance leads to surprisingly effective outcomes.',\n",
       " 'Currently, this approach has become the standard within the open-source community. In the industry as well, many practitioners utilize both human input and large language models (LLMs) to enhance the collection of human feedback data. This paper from last year illustrates that LLMs are now approximately fifty times more cost-effective than human annotators while demonstrating a higher level of agreement with human preferences than humans themselves. \\n\\nThis brings us to the evaluation of post-training models, which relates back to your initial question at the beginning of the lecture: how does one evaluate a model like ChatGPT? The potential responses that ChatGPT can generate are essentially unbounded, and it is important to note that there is not a singular correct answer; rather, there are numerous equally valid responses.\\n\\nSeveral challenges arise in this context. First, validation loss cannot be utilized effectively, as one method may employ Proximal Policy Optimization (PPO) while another may use Direct Preference Optimization (DPO), making validation loss non-comparable. Second, perplexity cannot be employed for evaluation purposes. As I mentioned earlier, these models are not calibrated; they do not provide probability distributions but instead optimize for a specific outcome. Consequently, perplexity is not a suitable metric for evaluating these models once they are aligned.\\n\\nThird, there exists a significant diversity in the types of questions that humans may pose to these models, encompassing various tasks such as open-ended question answering and summarization. This diversity complicates the evaluation process, as the tasks are inherently open-ended, making automation challenging. \\n\\nThus, rather than striving to create easily automated benchmarks, the approach we propose is to pose questions that users would realistically ask these models in practice. We will then instruct annotators to determine which of the two model outputs is superior. This methodology essentially mirrors the data collection process used in Reinforcement Learning from Human Feedback (RLHF), but it is now applied to the evaluation phase.\\n\\nI am not entirely clear on your point regarding the inability to use perplexity due to calibration issues. The LLM still performs next-token prediction; why is perplexity not applicable? Consider that the optimal solution following PPO results in a model that effectively produces a singular output for a given question. If this model encounters a question that is semantically similar but not identical, it may assign a likelihood of zero to that alternative response. While the situation is not as extreme as it may seem—since, as you noted, it still operates within a distribution—this highlights a fundamental issue with perplexity once these models are no longer functioning as traditional LLMs. Specifically, when trained with PPO, they are not designed to maximize likelihood in the conventional sense.',\n",
       " 'They were trained to be policies. The most common benchmark, or perhaps the most trusted one, is what we refer to as Chatbot Arena. This platform allows random users on the Internet to engage in blind conversations with two chatbots. Users pose various questions, evaluate the responses, and determine which answer is superior. This process is conducted with hundreds of thousands of users, resulting in actual preferences and rankings of the models.\\n\\nCurrently, one can visit Chatbot Arena and interact with these models. However, it is important to note that individuals participating in this type of evaluation are often more technologically inclined. Consequently, many of the questions posed tend to focus on technical topics, such as software errors and inquiries about AI tools. Another challenge is the cost and speed of utilizing this method for the development process, as it would require significant financial resources to compensate a large number of human evaluators.\\n\\nA straightforward alternative, as we have discussed previously, is to employ language models instead of human evaluators. The procedure involves generating outputs for each instruction from both a baseline model and the model under evaluation. For instance, if I am comparing responses from ChatGPT and Mistral, I would query another model to determine which response is better. I would then average the results across my entire dataset.\\n\\nThis approach yields a win rate, or win probability, for one model in comparison to another, allowing for the ranking of models. This methodology is exemplified by the Alpaca Eval leaderboard. The advantage of this approach is that it demonstrates a 98% correlation with Chatbot Arena, indicating a very high alignment with human evaluations. Additionally, it requires less than three minutes and costs under ten dollars to execute, making it a cost-effective solution.\\n\\nHowever, there are drawbacks. One notable issue is the poorer correlation observed in some instances. As previously mentioned, language models tend to favor longer outputs, and interestingly, humans also exhibit a preference for longer responses.',\n",
       " \"The issue that arises when utilizing large language models (LLMs) is that once bias is introduced, the optimization process will perpetuate that bias. For instance, if I pose a straightforward question and receive an excessively lengthy response, I am likely to express dissatisfaction with that answer. However, if LLMs have been trained with a bias toward longer outputs, they will continue to favor such responses. This illustrates a preference among both humans and models for more extensive outputs. \\n\\nFurthermore, I would like to present another perspective on the initial Alpaca evaluation dataset benchmark. When we assess GPT-4, specifically its win rate in comparison to itself, it achieves a baseline of 50% by definition, as we are comparing GPT-4 against GPT-4. However, when we prompt GPT-4 to be more verbose in its answers, its win rate increases to 64.4%. Conversely, when instructed to be concise, its win rate drops to 20%. This demonstrates a significant variance in performance based on the prompt's specificity regarding length, which can be quite frustrating.\\n\\nOne potential solution we implemented involved utilizing regression analysis. While I will not delve into the specifics, we employed causal inference tools to control for output length. Currently, the impact of length is considerably diminished; although prompting for verbosity still yields some gains, the effect is much less pronounced.\\n\\nNow, I will shift the focus to post-training for the next eight minutes, during which I can either discuss systems or address any questions. Yes, could you revisit your discussion on post-training? \\n\\nIn terms of post-training, how did we adjust those parameters using a limited dataset for fine-tuning, resulting in such a significant effect on the model? You previously mentioned that there is a distinct set of hyperparameters. Are we modifying only certain weights, specifically the later weights, or are we adjusting all the weights? What is actually occurring? Yes, I briefly touched upon this.\\n\\nIn practice, all weights are modified. In the industry, it is common to adjust all weights, whereas in the open-source domain, you may have encountered Low-Rank Adaptation (LoRA), which alters only a subset of the weights. More specifically, it introduces variations to the outputs of each layer. However, in industry settings, the approach typically involves fine-tuning all weights. Additionally, regarding the data, it is important to note that during the Reinforcement Learning from Human Feedback (RLHF) phase, one generally collects a significantly larger dataset compared to Supervised Fine-Tuning (SFT). While SFT might involve 5,000 to 10,000 examples, with RLHF, the dataset size often reaches the order of magnitude of 1 million.\",\n",
       " \"It is still significantly less than pre-training, which consists of 15 trillion tokens. This is a minuscule amount, yet it can greatly influence the model's weights. The learning rate employed during this process will differ, and it is important to consider how this is executed. \\n\\nFor instance, if I were to train on a single sentence repeatedly, my model would ultimately generate only that sentence, despite the vast amount of pre-training data. If a sufficiently large learning rate is applied for an extended period, the model will effectively overfit to that single sentence. \\n\\nThe key point to remember is that the data is not simply a mixture of post-training and pre-training data. Pre-training serves as the initialization of the model. Once you conceptualize it in this manner, recognizing that this is merely the initialization of weights, the prior training on extensive data becomes less significant. What truly matters is that you have established an initialization, and now you are engaged in actual training. \\n\\nConsider the Markov property in this context: you have your weights, which represent your initialization, and now you are training from that point. Does this address your question? \\n\\nSomewhat, but you mentioned that it is almost equivalent to rerunning the fine-tuning data multiple times. Is that indeed what occurs to enhance preference? \\n\\nAt present, I am uncertain how this is approached in the industry. During our work with Alpaca, we executed three epochs, meaning we ran it through the data three times. However, the frequency of iterations is not as critical as the effective learning rate, which is what truly matters. \\n\\nI believe I have five minutes remaining, correct? I will attempt to provide a high-level overview, at least from one of the systems we discussed. As we noted, the primary bottleneck is computational resources. One might inquire why we do not simply acquire more GPUs. \\n\\nGPUs are not only expensive but also scarce. Even with a budget of $10 million, it is challenging to procure the most advanced GPUs. Additionally, there are physical limitations when utilizing multiple GPUs, as communication between them becomes necessary.\",\n",
       " \"That takes time. Therefore, simply acquiring more GPUs is not a straightforward solution. It is crucial to consider how to allocate resources effectively and optimize your pipeline. In terms of GPU systems, I apologize for speaking at a slightly faster pace; I hope that at least some of you can keep up. GPUs are primarily optimized for throughput, whereas CPUs are optimized for latency. When considering GPUs, it is important to understand that a single command is executed across many cores simultaneously on various types of data. This is the fundamental characteristic of a GPU, which consists of numerous cores known as Streaming Multiprocessors, a structure that differs significantly from conventional CPU architecture. Thus, one should conceptualize GPUs in terms of high throughput parallelization.\\n\\nGPUs are particularly efficient for rapid matrix multiplication. Consequently, any operation performed on a GPU that can be expressed as a matrix multiplication will be approximately ten times faster than with alternative methods. This presents a limitation, as it confines us to operations that can be framed within the context of matrix multiplications. Additionally, it is noteworthy that computational capabilities have been advancing at a rate faster than memory and communication technologies. Currently, the data sent to GPUs often struggles to keep pace with processing demands, resulting in many GPUs remaining idle if one runs standard code without optimization.\\n\\nCommunication remains a critical factor, and this trend is likely to persist. Another important aspect of GPUs is their memory hierarchy, which is also applicable to CPUs. Essentially, the closer one is to the cores, the less memory is available, but the faster the operations can be executed. Conversely, greater memory availability at a distance from the cores results in slower performance. Regarding communication, the metric commonly examined is model FLOP utilization, which assesses the theoretical maximum performance of a GPU. This is calculated by dividing the observed throughput of operations by this theoretical maximum. Generally, achieving 50% utilization is considered satisfactory. For instance, I observed that Facebook's LLaMA model was operating at approximately 45%, indicating that data transfer rates are insufficient even for large organizations.\",\n",
       " 'One effective strategy, and perhaps the only one I will discuss, is the use of low precision. The fundamental idea is that by utilizing lower precision for floating-point numbers, fewer bits need to be transmitted to the GPUs. With a reduction in the number of bits, communication becomes faster, and memory consumption decreases, resulting in improved processing speed. In the context of deep learning, it is noteworthy that decimal precision is not critically important. \\n\\nFor instance, during matrix multiplication or when employing stochastic gradient descent (SGD), there is already a significant amount of noise present. Consequently, an update of 0.01 or 0.015 is often inconsequential. Therefore, instead of the traditional 32 bits per float, or even 64 bits as used in other domains, one can effectively use 16 bits for matrix multiplication. This means that for every floating-point number, only 16 bits are utilized. \\n\\nIn training, we often implement a technique known as automatic mixed precision, where some computations are performed in 32 bits while others are conducted in 16 bits. Generally, the weights of the model are stored in 32 bits, but prior to computation, all data is converted to 16 bits. This approach allows for rapid computation, and subsequently, the weights are updated in 32 bits. The rationale for performing updates in 32 bits is that, even with a very small learning rate, it is essential to ensure that changes in the weights can still be significant. Thus, while computations are executed in 16 bits, the weights remain stored in 32 bits, which has become the standard practice.\\n\\nNow, I will briefly address operator fusion, which I find particularly interesting. As previously mentioned, communication can be quite slow. Each time a line of code is executed in PyTorch, it typically transfers variables to the global memory of the GPU. For example, when executing a command such as `x.cosine()`, the data represented by `x` is sent to the GPU processors, the cosine function is applied, and the result is returned to the main memory of the GPU. Following this, the next line of code requires the data to be sent back to the GPU processors for another cosine operation, resulting in additional data transfers.',\n",
       " 'Another way to conceptualize this is to consider the process of transferring data from the dynamic random-access memory (DRAM), which serves as the global memory in the GPU, to the computation unit and then back for each line of data. This naive approach appears quite inefficient. The fundamental idea behind operator fusion is to minimize communication by performing all computations before transferring the results back. This is precisely what fused kernels accomplish. If you aim to enhance the speed of your computations in PyTorch, simply apply `torch.compile` to your model. This will approximately double the speed of your model. The process involves rewriting your PyTorch code in C or CUDA, allowing for communication to occur only once, followed by the execution of all operations before returning the results.\\n\\nI will not have time to discuss tiling, which is significant, as well as parallelization and the mixture of experts, both of which are also important concepts. \\n\\nLooking ahead, there are numerous topics we have yet to cover, including various architectures and inference methods. Additionally, we have not addressed the user interface (UI) utilized in these systems. Arguably, the primary innovation of ChatGPT was the introduction of a simple UI for user interaction. We must also consider multimodality and the potential misuses of these technologies, as well as the concern that there may not be sufficient data available on the Internet to train all these models. The legality of data collection presents another critical issue.\\n\\nFor those interested in these topics, I recommend three courses. CS224N is likely the one that addresses large language models (LLMs) the least, but it provides essential background and historical context regarding LLMs and related material. CS324, which I believe is titled Large Language Models, offers more in-depth readings and lectures on the subjects I have discussed. Lastly, CS336, which focuses on building large language models from scratch, allows students to create their own LLM. This is an exceptional course, taught by my two supervisors, but be aware that it entails a heavy workload.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rewritten_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final transcript and running summary saved to transcript_Standford_University_Building_LLMs.json\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 4) Output the final revised text and running summary as a JSON file\n",
    "final_text = \"\\n\".join(final_rewritten_text)\n",
    "\n",
    "data = {\n",
    "    \"final_text\": final_text,\n",
    "    \"running_summary\": running_summary\n",
    "}\n",
    "\n",
    "output_filename = f\"transcript_{job_name}.json\"\n",
    "try:\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"Final transcript and running summary saved to {output_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {str(e)}\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
